var suggestions=document.getElementById('suggestions'),userinput=document.getElementById('userinput');document.addEventListener('keydown',inputFocus);function inputFocus(a){a.keyCode===191&&(a.preventDefault(),userinput.focus()),a.keyCode===27&&(userinput.blur(),suggestions.classList.add('d-none'))}document.addEventListener('click',function(a){var b=suggestions.contains(a.target);b||suggestions.classList.add('d-none')}),document.addEventListener('keydown',suggestionFocus);function suggestionFocus(b){const d=suggestions.querySelectorAll('a'),e=[...d],a=e.indexOf(document.activeElement);let c=0;b.keyCode===38?(b.preventDefault(),c=a>0?a-1:0,d[c].focus()):b.keyCode===40&&(b.preventDefault(),c=a+1<e.length?a+1:a,d[c].focus())}(function(){var b=new FlexSearch({preset:'score',cache:!0,doc:{id:'id',field:['title','description','content'],store:['href','title','description']}}),c=[{id:0,href:"https://kubedl.io/docs/workloads/pytorch/",title:"PyTorch",description:"Run PyTorch job on Kubernetes.",content:'\u003ch2 id="example"\u003eExample\u003c/h2\u003e\n\u003cpre\u003e\u003ccode class="language-yaml"\u003eapiVersion: training.kubedl.io/v1alpha1\nkind: \u0026quot;PyTorchJob\u0026quot;\nmetadata:\n  name: \u0026quot;pytorch-dist-sendrecv-example\u0026quot;\n  namespace: \u0026quot;kubedl\u0026quot;\nspec:\n  pytorchReplicaSpecs:\n    Master:\n      replicas: 1\n      restartPolicy: ExitCode\n      template:\n        spec:\n          containers:\n            - name: pytorch\n              image: kubedl/pytorch-dist-example\n              imagePullPolicy: Always\n    Worker:\n      replicas: 2\n      restartPolicy: ExitCode\n      template:\n        spec:\n          containers:\n            - name: pytorch\n              image: kubedl/pytorch-dist-example\n              imagePullPolicy: Always\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2 id="crd-spec"\u003eCRD Spec\u003c/h2\u003e\n\u003cp\u003eCheck the CRD definition. \u003ca href="https://github.com/alibaba/kubedl/blob/master/apis/training/v1alpha1/pytorchjob_types.go"\u003eGo -\u0026gt;\u003c/a\u003e\u003c/p\u003e\n'},{id:1,href:"https://kubedl.io/docs/prologue/introduction/",title:"Introduction",description:"KubeDL runs your deep learning workloads on Kubernetes.",content:'\u003cp\u003eCurrently, KubeDL supports running \u003ca href="https://github.com/tensorflow/tensorflow"\u003eTensorFlow\u003c/a\u003e, \u003ca href="https://github.com/pytorch/pytorch"\u003ePyTorch\u003c/a\u003e,\n\u003ca href="https://github.com/dmlc/xgboost"\u003eXGBoost\u003c/a\u003e, \u003ca href="https://github.com/mars-project/mars"\u003eMars\u003c/a\u003e and MPI distributed training jobs on Kubernetes.\u003c/p\u003e\n\u003cfigure\u003e\n  \u003cimg class="img-fluid lazyload blur-up" data-sizes="auto" src="https://kubedl.io/docs/prologue/introduction/introduction_hufa79e617da320aec9287bf6111103a76_60530_20x0_resize_box_2.png" data-srcset="https://kubedl.io/docs/prologue/introduction/introduction_hufa79e617da320aec9287bf6111103a76_60530_900x0_resize_box_2.png 900w,https://kubedl.io/docs/prologue/introduction/introduction_hufa79e617da320aec9287bf6111103a76_60530_800x0_resize_box_2.png 800w,https://kubedl.io/docs/prologue/introduction/introduction_hufa79e617da320aec9287bf6111103a76_60530_700x0_resize_box_2.png 700w,https://kubedl.io/docs/prologue/introduction/introduction_hufa79e617da320aec9287bf6111103a76_60530_600x0_resize_box_2.png 600w,https://kubedl.io/docs/prologue/introduction/introduction_hufa79e617da320aec9287bf6111103a76_60530_500x0_resize_box_2.png 500w" width="706" height="531" alt="introduction"\u003e\n  \u003cnoscript\u003e\u003cimg class="img-fluid" sizes="100vw" srcset="https://kubedl.io/docs/prologue/introduction/introduction_hufa79e617da320aec9287bf6111103a76_60530_900x0_resize_box_2.png 900w,https://kubedl.io/docs/prologue/introduction/introduction_hufa79e617da320aec9287bf6111103a76_60530_800x0_resize_box_2.png 800w,https://kubedl.io/docs/prologue/introduction/introduction_hufa79e617da320aec9287bf6111103a76_60530_700x0_resize_box_2.png 700w,https://kubedl.io/docs/prologue/introduction/introduction_hufa79e617da320aec9287bf6111103a76_60530_600x0_resize_box_2.png 600w,https://kubedl.io/docs/prologue/introduction/introduction_hufa79e617da320aec9287bf6111103a76_60530_500x0_resize_box_2.png 500w" src="https://kubedl.io/docs/prologue/introduction/introduction.png" width="706" height="531" alt="introduction"\u003e\u003c/noscript\u003e\n  \u003cfigcaption class="figure-caption"\u003eIntroduction\u003c/figcaption\u003e\n\u003c/figure\u003e\n\n\u003ch2 id="key-features"\u003eKey Features\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eSupport different kinds of deep learning training jobs in a single controller. You don\u0026rsquo;t need to run each controller for each job kind.\u003c/li\u003e\n\u003cli\u003eExpose unified \u003ca href="https://kubedl.io/docs/references/metrics/"\u003eprometheus metrics\u003c/a\u003e for job stats.\u003c/li\u003e\n\u003cli\u003e\u003ca href="https://kubedl.io/docs/recipes/metadata-persistency/"\u003ePersist\u003c/a\u003e job metadata and events in external storage such as Mysql or certain event DB to outlive api-server state.\u003c/li\u003e\n\u003cli\u003e\u003ca href="https://kubedl.io/docs/recipes/code-sync/"\u003eSync files\u003c/a\u003e on container launch. You no longer need to rebuild the image to include the modified code every time.\u003c/li\u003e\n\u003cli\u003eRun jobs with \u003ca href="https://kubedl.io/docs/recipes/hostnetwork/"\u003ehost network\u003c/a\u003e for performance or nvlink communication across containers.\u003c/li\u003e\n\u003cli\u003eSupport advanced scheduling features such as gang scheduling.\u003c/li\u003e\n\u003cli\u003eSupport \u003ca href="https://kubedl.io/docs/recipes/tensorboard/"\u003eTensorboard\u003c/a\u003e out of the box.\u003c/li\u003e\n\u003cli\u003eA catchy \u003ca href="https://kubedl.io/docs/recipes/dashboard/"\u003edashboard\u003c/a\u003e !\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id="get-started"\u003eGet started\u003c/h2\u003e\n\u003cp\u003eThere are two main ways to install KubeDL.\u003c/p\u003e\n\u003ch3 id="install-using-helm"\u003eInstall using Helm\u003c/h3\u003e\n\u003cp\u003eInstall KubeDL using Helm charts. \u003ca href="https://kubedl.io/docs/prologue/install-using-helm/"\u003eGo →\u003c/a\u003e\u003c/p\u003e\n\u003ch3 id="install-using-yaml-files"\u003eInstall using YAML files\u003c/h3\u003e\n\u003cp\u003eInstall KubeDL using YAML files. \u003ca href="https://kubedl.io/docs/prologue/quick-start/"\u003eGo →\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id="recipes"\u003eRecipes\u003c/h2\u003e\n\u003cp\u003eGet instructions on how to accomplish common tasks with KubeDL. \u003ca href="https://getdoks.org/docs/recipes/project-configuration/"\u003eRecipes →\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id="reference"\u003eReference\u003c/h2\u003e\n\u003cp\u003eReferences for apis, metrics etc. \u003ca href="https://kubedl.io/docs/references/"\u003eReference →\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id="contributing"\u003eContributing\u003c/h2\u003e\n\u003cp\u003eFind out how to contribute to KubeDL. \u003ca href="https://kubedl.io/docs/contributing/how-to-contribute/"\u003eContributing →\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id="help"\u003eHelp\u003c/h2\u003e\n\u003cp\u003eGet help on KubeDL. \u003ca href="https://kubedl.io/docs/help/"\u003eHelp →\u003c/a\u003e\u003c/p\u003e\n'},{id:2,href:"https://kubedl.io/docs/prologue/",title:"Prologue",description:"Prologue KubeDL.",content:""},{id:3,href:"https://kubedl.io/docs/references/metrics/",title:"Metrics",description:"",content:"\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eMetric Names\u003c/th\u003e\n\u003cth\u003elabel\u003c/th\u003e\n\u003cth\u003eDescription\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003ekubedl_jobs_created\u003c/td\u003e\n\u003ctd\u003ekind\u003c/td\u003e\n\u003ctd\u003eCounts number of jobs created\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003ekubedl_jobs_deleted\u003c/td\u003e\n\u003ctd\u003ekind\u003c/td\u003e\n\u003ctd\u003eCounts number of jobs deleted\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003ekubedl_jobs_successful\u003c/td\u003e\n\u003ctd\u003ekind\u003c/td\u003e\n\u003ctd\u003eCounts number of jobs successfully finished\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003ekubedl_jobs_failed\u003c/td\u003e\n\u003ctd\u003ekind\u003c/td\u003e\n\u003ctd\u003eCounts number of jobs failed\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003ekubedl_jobs_restarted\u003c/td\u003e\n\u003ctd\u003ekind\u003c/td\u003e\n\u003ctd\u003eCounts number of jobs restarted\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003ekubedl_jobs_running\u003c/td\u003e\n\u003ctd\u003ekind\u003c/td\u003e\n\u003ctd\u003eCounts number of jobs currently running\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003ekubedl_jobs_pending\u003c/td\u003e\n\u003ctd\u003ekind\u003c/td\u003e\n\u003ctd\u003eCounts number of jobs currently pending\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003ekubedl_jobs_first_pod_launch_delay_seconds\u003c/td\u003e\n\u003ctd\u003ekind, name, namespace, uid\u003c/td\u003e\n\u003ctd\u003eHistogram for recording launch delay duration (from job created to first pod running)\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003ekubedl_jobs_all_pods_launch_delay_seconds\u003c/td\u003e\n\u003ctd\u003ekind, name, namespace, uid\u003c/td\u003e\n\u003ctd\u003eHistogram for recording launch delay duration (from job created to all pods running)\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u003ccode\u003elabel\u003c/code\u003e specifics the labels supported for the corresponding prometheus metrics\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003ekind\u003c/code\u003e - the target job kind, e.g. TFJob, PyTorchJob, MarsJob, XGBoostJob\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ename\u003c/code\u003e - the name of the job\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003enamespace\u003c/code\u003e - the namespace of the job\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003euid\u003c/code\u003e - the uid of the job\u003c/li\u003e\n\u003c/ul\u003e\n"},{id:4,href:"https://kubedl.io/docs/recipes/code-sync/",title:"File Sync",description:"Sync files on container launch.",content:'\u003cp\u003eKubeDL supports syncing files from remote on container launch.\nUser can modify the code, reference the code repository and run the jobs without re-building the image every time to include the modified code.\u003c/p\u003e\n\u003cp\u003eCurrently, only support downloading from github. The implementation is pluggable and can easily support other distributed filesystem like HDFS.\u003c/p\u003e\n\u003ch3 id="git-hub"\u003eGit Hub\u003c/h3\u003e\n\u003cp\u003eUsers can set the git config in the job\u0026rsquo;s annotation with key \u003ccode\u003ekubedl.io/git-sync-config\u003c/code\u003e as below. The git repo will be\ndownloaded and saved in the container\u0026rsquo;s \u003ccode\u003eworking dir\u003c/code\u003e by default. Please use the git repo\u0026rsquo;s clone url ending with the \u003ccode\u003e.git\u003c/code\u003e,\nrather than the git repo\u0026rsquo;s web url.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-yaml"\u003e    apiVersion: \u0026quot;training.kubedl.io/v1alpha1\u0026quot;\n    kind: \u0026quot;TFJob\u0026quot;\n    metadata:\n      name: \u0026quot;mnist\u0026quot;\n      namespace: kubedl\n      annotations:\n +      kubedl.io/git-sync-config: \'{\u0026quot;source\u0026quot;: \u0026quot;https://github.com/alibaba/kubedl.git\u0026quot; }\'\n    spec:\n      cleanPodPolicy: None\n      tfReplicaSpecs:\n        ...\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eA full list of supported options are:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-json"\u003e{\n    \u0026quot;source\u0026quot;: \u0026quot;https://github.com/sample/sample.git\u0026quot;,  // code source (required).\n    \u0026quot;image\u0026quot;: \u0026quot;xxx\u0026quot;,     // the image to execute the git-sync logic (optional).\n    \u0026quot;rootPath\u0026quot;: \u0026quot;xxx\u0026quot;,  // the path to save downloaded files (optional).\n    \u0026quot;destPath\u0026quot;: \u0026quot;xxx\u0026quot;,  // the name of (a symlink to) a directory in which to check-out files (optional).\n    \u0026quot;envs\u0026quot;: [],         // user-customized environment variables (optional).\n    \u0026quot;branch\u0026quot;: \u0026quot;xxx\u0026quot;,    // git repo branch (optional).\n    \u0026quot;revison\u0026quot;: \u0026quot;xxx\u0026quot;,   // git repo commit revision (optional).\n    \u0026quot;depth\u0026quot;: \u0026quot;xxx\u0026quot;,     // git sync depth (optional).\n    \u0026quot;maxFailures\u0026quot; : 3,  // max consecutive failures allowed (optional).\n    \u0026quot;ssh\u0026quot;: false,       // use ssh mode or not (optional).\n    \u0026quot;sshFile\u0026quot;: \u0026quot;xxx\u0026quot;,   // ssh file path (optional).\n    \u0026quot;user\u0026quot;: \u0026quot;xxx\u0026quot;,      // git config username (optional).\n    \u0026quot;password\u0026quot;: \u0026quot;xxx\u0026quot;   // git config password (optional).\n}\n\u003c/code\u003e\u003c/pre\u003e\n'},{id:5,href:"https://kubedl.io/docs/recipes/hostnetwork/",title:"Host Network",description:"Run jobs with host network",content:'\u003ch2 id="background"\u003eBackground\u003c/h2\u003e\n\u003cp\u003eNetwork bandwidth is a bottleneck resource for communication-intensive jobs. Host mode networking can be useful to optimize\nperformance. In addition, other scenarios (e.g: nvlink communications between containerized gpu processes) may depend on\nhost-network as well.\u003c/p\u003e\n\u003ch2 id="how-to-use"\u003eHow To Use\u003c/h2\u003e\n\u003cp\u003eKubeDL provides a feature-gate to enable \u003ccode\u003ehostnetwork\u003c/code\u003e mode for jobs. Users only need to add an annotation\n\u003ccode\u003ekubedl.io/network-mode: host\u003c/code\u003e to the job specifications, for example:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-yaml"\u003e    apiVersion: \u0026quot;training.kubedl.io/v1alpha1\u0026quot;\n    kind: \u0026quot;TFJob\u0026quot;\n    metadata:\n      name: \u0026quot;mnist\u0026quot;\n      namespace: kubedl\n      annotations:\n +      kubedl.io/network-mode: \'host\'\n    spec:\n      cleanPodPolicy: None\n      tfReplicaSpecs:\n        Worker:\n          replicas: 3\n          ...\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2 id="design"\u003eDesign\u003c/h2\u003e\n\u003cp\u003eThe essence of \u003ccode\u003ehostnetwork-mode\u003c/code\u003e is to randomize container ports to avoid port collision and enable service discovery\nacross workers. KubeDL achieves by following steps:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eEnable \u003ccode\u003ehostnetwork\u003c/code\u003e in \u003ccode\u003ePod\u003c/code\u003e spec and set DNS policy as \u003ccode\u003eClusterFirstWithHostNet\u003c/code\u003e;\u003c/li\u003e\n\u003cli\u003eChoose a random port as container port.\u003c/li\u003e\n\u003cli\u003eChange \u003ccode\u003eTargetPort\u003c/code\u003e of corresponding worker\u0026rsquo;s \u003ccode\u003eService\u003c/code\u003e to the previous randomized port, and set \u003ccode\u003eCluterIP\u003c/code\u003e as empty string(instead of \u003ccode\u003eNone\u003c/code\u003e), so that kube-proxy will be able to forward traffic from \u003ccode\u003ePort\u003c/code\u003e to \u003ccode\u003eTargetPort\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003eChange the job cluster spec (e.g. the \u003ccode\u003eTF_CONFIG\u003c/code\u003e) .\u003c/li\u003e\n\u003cli\u003eHandle worker fail-over and use latest available port as the \u003ccode\u003eTargetPort\u003c/code\u003e in the new worker.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eHere is a Tensorflow job example:\u003c/p\u003e\n\u003cfigure\u003e\n  \u003cimg class="img-fluid lazyload blur-up" data-sizes="auto" src="https://kubedl.io/docs/recipes/hostnetwork/tf_hostnetwork_hu3231db9fe621f8cd4234a60c2aad43b9_63572_20x0_resize_box_2.png" data-srcset="https://kubedl.io/docs/recipes/hostnetwork/tf_hostnetwork_hu3231db9fe621f8cd4234a60c2aad43b9_63572_900x0_resize_box_2.png 900w,https://kubedl.io/docs/recipes/hostnetwork/tf_hostnetwork_hu3231db9fe621f8cd4234a60c2aad43b9_63572_800x0_resize_box_2.png 800w,https://kubedl.io/docs/recipes/hostnetwork/tf_hostnetwork_hu3231db9fe621f8cd4234a60c2aad43b9_63572_700x0_resize_box_2.png 700w,https://kubedl.io/docs/recipes/hostnetwork/tf_hostnetwork_hu3231db9fe621f8cd4234a60c2aad43b9_63572_600x0_resize_box_2.png 600w,https://kubedl.io/docs/recipes/hostnetwork/tf_hostnetwork_hu3231db9fe621f8cd4234a60c2aad43b9_63572_500x0_resize_box_2.png 500w" width="912" height="472" alt="tf_hostnetwork"\u003e\n  \u003cnoscript\u003e\u003cimg class="img-fluid" sizes="100vw" srcset="https://kubedl.io/docs/recipes/hostnetwork/tf_hostnetwork_hu3231db9fe621f8cd4234a60c2aad43b9_63572_900x0_resize_box_2.png 900w,https://kubedl.io/docs/recipes/hostnetwork/tf_hostnetwork_hu3231db9fe621f8cd4234a60c2aad43b9_63572_800x0_resize_box_2.png 800w,https://kubedl.io/docs/recipes/hostnetwork/tf_hostnetwork_hu3231db9fe621f8cd4234a60c2aad43b9_63572_700x0_resize_box_2.png 700w,https://kubedl.io/docs/recipes/hostnetwork/tf_hostnetwork_hu3231db9fe621f8cd4234a60c2aad43b9_63572_600x0_resize_box_2.png 600w,https://kubedl.io/docs/recipes/hostnetwork/tf_hostnetwork_hu3231db9fe621f8cd4234a60c2aad43b9_63572_500x0_resize_box_2.png 500w" src="https://kubedl.io/docs/recipes/hostnetwork/tf_hostnetwork.png" width="912" height="472" alt="tf_hostnetwork"\u003e\u003c/noscript\u003e\n  \u003cfigcaption class="figure-caption"\u003etensorflow hostnetwork\u003c/figcaption\u003e\n\u003c/figure\u003e\n\n'},{id:6,href:"https://kubedl.io/docs/workloads/tensorflow/",title:"TensorFlow",description:"Run Tensorflow on Kubernetes.",content:'\u003ch2 id="example"\u003eExample\u003c/h2\u003e\n\u003cpre\u003e\u003ccode class="language-yaml"\u003eapiVersion: training.kubedl.io/v1alpha1\nkind: \u0026quot;TFJob\u0026quot;\nmetadata:\n  name: \u0026quot;mnist\u0026quot;\n  namespace: kubedl\nspec:\n  cleanPodPolicy: None\n  tfReplicaSpecs:\n    Worker:\n      replicas: 1\n      restartPolicy: Never\n      template:\n        spec:\n          containers:\n            - name: tensorflow\n              image: kubedl/tf-mnist-with-summaries:1.0\n              command:\n                - \u0026quot;python\u0026quot;\n                - \u0026quot;/var/tf_mnist/mnist_with_summaries.py\u0026quot;\n                - \u0026quot;--log_dir=/train/logs\u0026quot;\n                - \u0026quot;--learning_rate=0.01\u0026quot;\n                - \u0026quot;--batch_size=150\u0026quot;\n              volumeMounts:\n                - mountPath: \u0026quot;/train\u0026quot;\n                  name: \u0026quot;training\u0026quot;\n              resources:\n                limits:\n                  cpu: 2048m\n                  memory: 2Gi\n                requests:\n                  cpu: 1024m\n                  memory: 1Gi\n          volumes:\n            - name: \u0026quot;training\u0026quot;\n              hostPath:\n                path: /tmp/data\n                type: DirectoryOrCreate\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2 id="crd-spec"\u003eCRD Spec\u003c/h2\u003e\n\u003cp\u003eCheck the CRD definition. \u003ca href="https://github.com/alibaba/kubedl/blob/master/apis/training/v1alpha1/tfjob_types.go"\u003eGo -\u0026gt;\u003c/a\u003e\u003c/p\u003e\n'},{id:7,href:"https://kubedl.io/docs/references/dockerhub/",title:"Docker Images",description:"Docker Images",content:'\u003cp\u003eKubeDL controller docker images and are hosted at \u003ca href="https://hub.docker.com/r/kubedl/kubedl"\u003edocker hub\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eYou can also find other examples\' images at \u003ca href="https://hub.docker.com/r/kubedl/"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n'},{id:8,href:"https://kubedl.io/docs/references/flags/",title:"Startup flags",description:"KubeDL startup flags",content:"\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eFlag Name\u003c/th\u003e\n\u003cth\u003eDescription\u003c/th\u003e\n\u003cth\u003eDefault\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003econtroller-metrics-addr\u003c/td\u003e\n\u003ctd\u003eThe prometheus metrics endpoint for job stats\u003c/td\u003e\n\u003ctd\u003e8088\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eenable-leader-election\u003c/td\u003e\n\u003ctd\u003eEnable leader election for controller manager. Enabling this will ensure there is only one active controller manager.\u003c/td\u003e\n\u003ctd\u003efalse\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003egang-scheduler-name\u003c/td\u003e\n\u003ctd\u003eThe name of gang scheduler, by default it is set to empty meaning not enabling gang scheduling.Empty means not enabling gang scheduling. Supported values are kube-coscheduler, kube-batch.\u003c/td\u003e\n\u003ctd\u003e\u0026quot;\u0026quot;\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003emax-reconciles\u003c/td\u003e\n\u003ctd\u003eThe number of max concurrent reconciles of each controller\u003c/td\u003e\n\u003ctd\u003e1\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n"},{id:9,href:"https://kubedl.io/docs/references/crd/",title:"Training CRD Spec",description:"KubeDL Training Job CRD Spec",content:'\u003ch2 id="tensorflow-crd"\u003eTensorFlow CRD\u003c/h2\u003e\n\u003cp\u003e\u003ca href="https://github.com/kubedl-io/kubedl/blob/master/apis/training/v1alpha1/tfjob_types.go"\u003eGo -\u0026gt;\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id="pytorch-crd"\u003ePyTorch CRD\u003c/h2\u003e\n\u003cp\u003e\u003ca href="https://github.com/kubedl-io/kubedl/blob/master/apis/training/v1alpha1/pytorchjob_types.go"\u003eGo -\u0026gt;\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id="mars-crd"\u003eMars CRD\u003c/h2\u003e\n\u003cp\u003e\u003ca href="https://github.com/kubedl-io/kubedl/blob/master/apis/training/v1alpha1/marsjob_types.go"\u003eGo -\u0026gt;\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id="mpi-crd"\u003eMPI CRD\u003c/h2\u003e\n\u003cp\u003e\u003ca href="https://github.com/kubedl-io/kubedl/blob/master/apis/training/v1alpha1/mpijob_types.go"\u003eGo -\u0026gt;\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id="xgboost-crd"\u003eXGBoost CRD\u003c/h2\u003e\n\u003cp\u003e\u003ca href="https://github.com/kubedl-io/kubedl/blob/master/apis/training/v1alpha1/xgboostjob_types.go"\u003eGo -\u0026gt;\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id="cron-crd"\u003eCron CRD\u003c/h2\u003e\n\u003cp\u003e\u003ca href="https://github.com/kubedl-io/kubedl/blob/master/apis/apps/v1alpha1/cron_types.go"\u003eGo -\u0026gt;\u003c/a\u003e\u003c/p\u003e\n'},{id:10,href:"https://kubedl.io/docs/tutorial/tutorial/",title:"Tutorial",description:"An end-to-end tutorial from model training to serving.",content:'\u003cp\u003eThis tutorial will walk through \u003ca href="https://kubedl.io/docs/prologue/introduction/"\u003eKubeDL Training\u003c/a\u003e, \u003ca href="https://kubedl.io/model/intro/"\u003eKubeDL Model\u003c/a\u003e and \u003ca href="https://kubedl.io/serving/intro/"\u003eKubeDL Serving\u003c/a\u003e concepts.\u003c/p\u003e\n\u003ch2 id="install-kubedl"\u003eInstall KubeDL\u003c/h2\u003e\n\u003cp\u003eFollow the instructions to install KubeDL. \u003ca href="https://kubedl.io/docs/prologue/install-using-helm/"\u003eGo →\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id="setup-docker-credential"\u003eSetup docker credential\u003c/h2\u003e\n\u003cp\u003eThe docker credential is required for KubeDL Model to store the model image.\nThis step is not needed if you don\u0026rsquo;t use KubeDL model. This tutorial will use it.\u003c/p\u003e\n\u003cp\u003eFollow the setup instruction to setup docker credential. \u003ca href="https://kubedl.io/model/setup/"\u003eGo →\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id="run-a-job-to-train-a-model"\u003eRun a job to train a model\u003c/h2\u003e\n\u003cp\u003eThis example trains a mnist model using distributed TensorFlow. From project root, run:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003ekubectl apply -f example/tf/tf_job_mnist_modelversion.yaml\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eExplanation on the \u003ca href="https://github.com/kubedl-io/kubedl/blob/master/example/tf/tf_job_mnist_model.yaml"\u003eYAML\u003c/a\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-yaml"\u003eapiVersion: \u0026quot;training.kubedl.io/v1alpha1\u0026quot;\nkind: \u0026quot;TFJob\u0026quot;\nmetadata:\n  name: \u0026quot;tf-distributed\u0026quot;\nspec:\n  cleanPodPolicy: None\n  # modelVersion defines the location where the model is stored.\n  modelVersion:\n    modelName: mymodel\n    # The dockerhub repo to push the generated image\n    imageRepo: jianhe6/mymodel\n    storage:\n      # Use hostpath, NFS is also supported.\n      localStorage:\n        # The host dir for THIS model version, each modelVersion should have its own unique parent folder, in this case, \'mymodelv1\'\n        path: /models/mymodelv1\n        # The node for storing the model\n        nodeName: kind-control-plane\n  tfReplicaSpecs:\n    Worker:\n      replicas: 3\n      restartPolicy: Never\n      template:\n        spec:\n          containers:\n            - name: tensorflow\n              image: kubedl/tf-mnist-estimator-api:v0.1\n              imagePullPolicy: Always\n              command:\n                - \u0026quot;python\u0026quot;\n                - \u0026quot;/keras_model_to_estimator.py\u0026quot;\n                - \u0026quot;/tmp/tfkeras_example/\u0026quot; # model checkpoint dir\n                - \u0026quot;/kubedl-model\u0026quot;         # export dir for the saved_model format\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis example stores the trained model at a hostpath, in this case \u003ccode\u003e/models/mymodelv1\u003c/code\u003e. KubeDL will automatically\ncreate a container image that includes the model artifacts within that folder and push that to the dockerhub.\u003c/p\u003e\n\u003cp\u003eNotes：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003ccode\u003emodelVersion\u003c/code\u003e field defines where the model is stored. Currently, local hostpath and NFS are supported.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e/kubedl-model\u003c/code\u003e is the pre-defined location for storing the trained model inside the model image. The training code is expected to export the model in this location.\nCheck the \u003ca href="https://kubedl.io/model/usage/"\u003edocumentation\u003c/a\u003e for more details\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id="inspect-the-job"\u003eInspect the job\u003c/h2\u003e\n\u003cp\u003eAfter the job succeeded, run \u003ccode\u003ekubect get tfjob\u003c/code\u003e:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003eNAME              STATE       AGE   TTL-AFTER-FINISHED   MAX-LIFETIME   MODEL-VERSION\ntf-distributed    Succeeded   45s                                       model-version-tf-distributed\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eHere, a ModelVersion named \u003ccode\u003emodel-version-tf-distributed\u003c/code\u003e is created.\u003c/p\u003e\n\u003cp\u003eThe naming convention of the ModelVersion is to prepend the \u003ccode\u003emodel-version\u003c/code\u003e to the job name as such: \u003ccode\u003emodel-version-\u0026lt;JobName\u0026gt;\u003c/code\u003e\u003c/p\u003e\n\u003ch2 id="inspect-the-model-version"\u003eInspect the model version\u003c/h2\u003e\n\u003cp\u003eThere are \u003ca href="https://kubedl.io/model/intro/"\u003e\u003ccode\u003eModel\u003c/code\u003e and \u003ccode\u003eModelVersion\u003c/code\u003e\u003c/a\u003e . In short, \u003ccode\u003eModel\u003c/code\u003e associates with a series of \u003ccode\u003eModelVersion\u003c/code\u003es.\u003c/p\u003e\n\u003cp\u003eInspect the model version\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003e$ kubectl get mv\nNAME                           MODEL     IMAGE                    CREATED-BY       FINISH-TIME\nmodel-version-tf-distributed   mymodel   jianhe6/mymodel:vf812c   tf-distributed   2021-07-24T00:39:02Z\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eModelVersion \u003ccode\u003emodel-version-tf-distributed\u003c/code\u003e belongs to model \u003ccode\u003emymodel\u003c/code\u003e. The image incorporating the model is \u003ccode\u003ejianhe6/mymodel:vf812c\u003c/code\u003e.\nIt is created by job \u003ccode\u003etf-distributed\u003c/code\u003e at \u003ccode\u003e2021-07-24T00:39:02Z\u003c/code\u003e\u003c/p\u003e\n\u003ch2 id="run-the-job-again"\u003eRun the job again\u003c/h2\u003e\n\u003cp\u003eThis time we made some changes to the code, and we change the job name to \u003ccode\u003etf-distributed-2\u003c/code\u003e and\nchange the \u003ccode\u003elocalstorage.path\u003c/code\u003e to \u003ccode\u003e/models/mymodelv2\u003c/code\u003e, so that the new model version will be stored at hostpath \u003ccode\u003e/models/mymodelv2\u003c/code\u003e,\nand run it again.\u003c/p\u003e\n\u003cp\u003eOnce the job finishes, we get below output:\u003c/p\u003e\n\u003cp\u003eTwo jobs, each has its own model version.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003e$ kubectl get tfjob\nNAME               STATE       AGE     TTL-AFTER-FINISHED   MAX-LIFETIME   MODEL-VERSION\ntf-distributed     Succeeded   3m57s                                       model-version-tf-distributed\ntf-distributed-2   Succeeded   24s                                         model-version-tf-distributed-2\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eCheck the model version. We get two. Each has its own generated model image.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003e$ kubectl get mv\nNAME                             MODEL     IMAGE                    CREATED-BY         FINISH-TIME\nmodel-version-tf-distributed     mymodel   jianhe6/mymodel:vf812c   tf-distributed     2021-07-24T00:39:02Z\nmodel-version-tf-distributed-2   mymodel   jianhe6/mymodel:vc73be   tf-distributed-2   2021-07-24T00:42:28Z\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNow, check the model,\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003e$ kubectl get model\nNAME      LATEST-VERSION                   LATEST-IMAGE\nmymodel   model-version-tf-distributed-2   jianhe6/mymodel:vc73be\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe model named \u003ccode\u003emymodel\u003c/code\u003e, has the latest model version named \u003ccode\u003e model-version-tf-distributed-2\u003c/code\u003e, and its latest model image is \u003ccode\u003ejianhe6/mymodel:vc73be\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eDeleting the model will  delete all its model versions.\u003c/p\u003e\n\u003ch2 id="inspect-the-model-image-content"\u003eInspect the model image content\u003c/h2\u003e\n\u003cp\u003eDownload the model image and inspect its contents. The model artifacts will be under \u003ccode\u003e/kubedl-model\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eRun \u003ccode\u003edocker run -it jianhe6/mymodel:vc73be /bin/sh\u003c/code\u003e, and then run \u003ccode\u003els /kubedl-model\u003c/code\u003e inside the container:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003e$ ls /kubedl-model/1627086141\nassets          saved_model.pb  variables\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eHere, the \u003ccode\u003e1627086141/\u003c/code\u003e parent folder is automatically created by the training code library.\u003c/p\u003e\n\u003ch2 id="serve-the-model"\u003eServe the model\u003c/h2\u003e\n\u003cp\u003eRun an Inference workload\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003ekubectl apply -f example/tf/tf_serving_modelversion.yaml\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe YAML content looks like below:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-yaml"\u003eapiVersion: serving.kubedl.io/v1alpha1\nkind: Inference\nmetadata:\n  name: hello-inference\nspec:\n  framework: TFServing\n  predictors:\n    - name: model-predictor\n      # The model version to be served\n      modelVersion: mv-tf-distributed-5f4c7\n      # The model path inside the container, it\'s the tensorflow serving model_base_path\n      modelPath: /kubedl-model\n      replicas: 1\n      batching:\n        batchSize: 32\n      template:\n        spec:\n          containers:\n            - name: tensorflow\n              args:\n                - --port=9000\n                - --rest_api_port=8500\n                - --model_name=mnist\n                - --model_base_path=/kubedl-model/  # This should be the same as modelPath field.\n              command:\n                - /usr/bin/tensorflow_model_server\n              image: tensorflow/serving:1.11.1\n              imagePullPolicy: IfNotPresent\n              ports:\n                - containerPort: 9000\n                - containerPort: 8500\n              resources:\n                limits:\n                  cpu: 2048m\n                  memory: 2Gi\n                requests:\n                  cpu: 1024m\n                  memory: 1Gi\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eInspect the Inference workload state\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003ekubectl get inference\n\u003c/code\u003e\u003c/pre\u003e\n'},{id:11,href:"https://kubedl.io/docs/contributing/how-to-contribute/",title:"How to Contribute",description:"You are very welcome to contribute to KubeDL",content:'\u003ch1 id="contributing-to-kubedl"\u003eContributing to KubeDL\u003c/h1\u003e\n\u003cp\u003eWelcome to KubeDL!\nWe encourage you to help out by reporting issues, improving documentation, fixing bugs, or adding new features.\nPlease also take a look at our code of conduct, which details how contributors are expected to conduct themselves as part of the KubeDL community.\u003c/p\u003e\n\u003ch2 id="developing-environment"\u003eDeveloping Environment\u003c/h2\u003e\n\u003cp\u003eAs a contributor, if you want to make any contribution to KubeDL project, we should reach an agreement on the version of tools used in the development environment.\nHere are some dependents with specific version:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eGolang : v1.13+ (1.14 is best)\u003c/li\u003e\n\u003cli\u003eKubernetes: v1.12+\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id="developing-guide"\u003eDeveloping guide\u003c/h2\u003e\n\u003ch3 id="enable-dco"\u003eEnable DCO\u003c/h3\u003e\n\u003cp\u003eKubeDL has enabled \u003ca href="https://github.com/apps/dco"\u003eDCO\u003c/a\u003e.\nThus, you will need to sign-off your commits in your pull requests.\nGit has a \u003ccode\u003e-s\u003c/code\u003e command line option that can sign-off your commit automatically:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003egit commit -s -m \'This is my commit message\'\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3 id="code-structure"\u003eCode Structure\u003c/h3\u003e\n\u003cp\u003eKubeDL uses \u003ca href="https://github.com/kubernetes-sigs/kubebuilder"\u003eKubeBuilder\u003c/a\u003e for scaffolding code.\u003c/p\u003e\n\u003cp\u003eKubeDL code base consists of several components:\u003c/p\u003e\n\u003cfigure class="border-0 rounded-circle"\u003e\n  \u003cimg class="img-fluid lazyload blur-up" data-sizes="auto" src="https://kubedl.io/docs/contributing/how-to-contribute/codestructure_hua51bab0724f047edbd3d9df2d0503bb0_159380_20x0_resize_q75_box.jpg" data-srcset="https://kubedl.io/docs/contributing/how-to-contribute/codestructure_hua51bab0724f047edbd3d9df2d0503bb0_159380_800x0_resize_q75_box.jpg 800w,https://kubedl.io/docs/contributing/how-to-contribute/codestructure_hua51bab0724f047edbd3d9df2d0503bb0_159380_700x0_resize_q75_box.jpg 700w,https://kubedl.io/docs/contributing/how-to-contribute/codestructure_hua51bab0724f047edbd3d9df2d0503bb0_159380_600x0_resize_q75_box.jpg 600w,https://kubedl.io/docs/contributing/how-to-contribute/codestructure_hua51bab0724f047edbd3d9df2d0503bb0_159380_500x0_resize_q75_box.jpg 500w" width="760" height="1164" alt="code"\u003e\n  \u003cnoscript\u003e\u003cimg class="img-fluid" sizes="100vw" srcset="https://kubedl.io/docs/contributing/how-to-contribute/codestructure_hua51bab0724f047edbd3d9df2d0503bb0_159380_800x0_resize_q75_box.jpg 800w,https://kubedl.io/docs/contributing/how-to-contribute/codestructure_hua51bab0724f047edbd3d9df2d0503bb0_159380_700x0_resize_q75_box.jpg 700w,https://kubedl.io/docs/contributing/how-to-contribute/codestructure_hua51bab0724f047edbd3d9df2d0503bb0_159380_600x0_resize_q75_box.jpg 600w,https://kubedl.io/docs/contributing/how-to-contribute/codestructure_hua51bab0724f047edbd3d9df2d0503bb0_159380_500x0_resize_q75_box.jpg 500w" src="https://kubedl.io/docs/contributing/how-to-contribute/codestructure.jpg" width="760" height="1164" alt="code"\u003e\u003c/noscript\u003e\n  \n\u003c/figure\u003e\n\n\u003ch3 id="how-to-build"\u003eHow to Build\u003c/h3\u003e\n\u003cp\u003eThere\u0026rsquo;s a \u003ccode\u003eMakefile\u003c/code\u003e in the root folder which describes the options to build and install. Here are some common ones:\u003c/p\u003e\n\u003ch4 id="build-the-binary"\u003eBuild the binary\u003c/h4\u003e\n\u003cdiv class="doks-clipboard"\u003e\n  \u003cbutton class="btn-clipboard btn btn-link" data-clipboard-text="make manager"\u003e\u003cspan class="copy-status"\u003e\u003c/span\u003e\u003c/button\u003e\n\u003c/div\u003e\n\n\u003cpre\u003e\u003ccode class="language-bash"\u003emake manager\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4 id="run-the-tests"\u003eRun the tests\u003c/h4\u003e\n\u003cdiv class="doks-clipboard"\u003e\n  \u003cbutton class="btn-clipboard btn btn-link" data-clipboard-text="make test"\u003e\u003cspan class="copy-status"\u003e\u003c/span\u003e\u003c/button\u003e\n\u003c/div\u003e\n\n\u003cpre\u003e\u003ccode class="language-bash"\u003emake test\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4 id="generate-manifests-crd-rbac-yaml-files-etc"\u003eGenerate manifests: CRD, RBAC YAML files etc\u003c/h4\u003e\n\u003cdiv class="doks-clipboard"\u003e\n  \u003cbutton class="btn-clipboard btn btn-link" data-clipboard-text="make manifests"\u003e\u003cspan class="copy-status"\u003e\u003c/span\u003e\u003c/button\u003e\n\u003c/div\u003e\n\n\u003cpre\u003e\u003ccode class="language-bash"\u003emake manifests\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4 id="build-the-docker-image"\u003eBuild the docker image\u003c/h4\u003e\n\u003cp\u003eReplace the image name to your own image\u003c/p\u003e\n\u003cdiv class="doks-clipboard"\u003e\n  \u003cbutton class="btn-clipboard btn btn-link" data-clipboard-text="export IMG=kubedl/kubedl:v0.3.0 \u0026\u0026 make docker-build"\u003e\u003cspan class="copy-status"\u003e\u003c/span\u003e\u003c/button\u003e\n\u003c/div\u003e\n\n\u003cpre\u003e\u003ccode class="language-bash"\u003eexport IMG=kubedl/kubedl:v0.3.0 \u0026amp;\u0026amp; make docker-build\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4 id="push-the-image"\u003ePush the image\u003c/h4\u003e\n\u003cp\u003eChange the image name to your own image.\n\u003cdiv class="doks-clipboard"\u003e\n  \u003cbutton class="btn-clipboard btn btn-link" data-clipboard-text="docker push kubedl/kubedl:v0.3.0"\u003e\u003cspan class="copy-status"\u003e\u003c/span\u003e\u003c/button\u003e\n\u003c/div\u003e\n\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003edocker push kubedl/kubedl:v0.3.0\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4 id="generate-the-helm-chart"\u003eGenerate the helm chart\u003c/h4\u003e\n\u003cp\u003eGenerate the helm chart, the helm chart will be generated under \u003ca href="https://github.com/kubedl-io/kubedl/tree/master/helm/kubedl"\u003ehelm/kubedl\u003c/a\u003e\n\u003cdiv class="doks-clipboard"\u003e\n  \u003cbutton class="btn-clipboard btn btn-link" data-clipboard-text="make helm-chart"\u003e\u003cspan class="copy-status"\u003e\u003c/span\u003e\u003c/button\u003e\n\u003c/div\u003e\n\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003emake helm-chart\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3 id="git-preparation-skip-if-you-are-familiar-with-git"\u003eGit Preparation (Skip if you are familiar with Git)\u003c/h3\u003e\n\u003cp\u003eTo put forward a PR, we assume you have registered a GitHub ID.\nThen you could finish the preparation in the following steps:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eFork\u003c/strong\u003e the repository you wish to work on. You just need to click the button Fork in right-left of project repository main page. Then you will end up with your repository in your GitHub username.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eClone\u003c/strong\u003e your own repository to develop locally. Use \u003ccode\u003egit clone https://github.com/\u0026lt;your-username\u0026gt;/kubedl.git\u003c/code\u003e to clone repository to your local machine. Then you can create new branches to finish the change you wish to make.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSet remote\u003c/strong\u003e upstream to be \u003ccode\u003ehttps://github.com/kubel-io/kubedl.git\u003c/code\u003e using the following two commands:\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003egit remote add upstream https://github.com/kubedl-io/kubedl.git\ngit remote set-url --push upstream no-pushing\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAdding this, we can easily synchronize local branches with upstream branches.\u003c/p\u003e\n\u003col start="4"\u003e\n\u003cli\u003e\u003cstrong\u003eCreate a branch\u003c/strong\u003e to add a new feature or fix issues\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eUpdate local working directory:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003ecd \u0026lt;project\u0026gt;\ngit fetch upstream\ngit checkout master\ngit rebase upstream/master\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eCreate a new branch:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003egit checkout -b \u0026lt;new-branch\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eMake any change on the new-branch then build and test your codes.\u003c/p\u003e\n\u003ch2 id="engage-to-help-anything"\u003eEngage to help anything\u003c/h2\u003e\n\u003ch3 id="reporting-issues"\u003eReporting issues\u003c/h3\u003e\n\u003cp\u003eWe regard every user of KubeDL as a very kind contributor.\nAfter experiencing KubeDL, you may have some feedback for the project.\nThen feel free to open an issue.\u003c/p\u003e\n\u003cp\u003eThere are lot of cases when you could open an issue:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ebug report\u003c/li\u003e\n\u003cli\u003efeature request\u003c/li\u003e\n\u003cli\u003eperformance issues\u003c/li\u003e\n\u003cli\u003efeature proposal\u003c/li\u003e\n\u003cli\u003efeature design\u003c/li\u003e\n\u003cli\u003ehelp wanted\u003c/li\u003e\n\u003cli\u003edoc incomplete\u003c/li\u003e\n\u003cli\u003etest improvement\u003c/li\u003e\n\u003cli\u003eany questions on project\u003c/li\u003e\n\u003cli\u003eand so on\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAlso we must remind that when filing a new issue, please remember to remove the sensitive data from your post.\nSensitive data could be password, secret key, network locations, private business data and so on.\u003c/p\u003e\n\u003ch3 id="code-and-doc-contribution"\u003eCode and doc contribution\u003c/h3\u003e\n\u003cp\u003eThe KubeDL website repo is hosted at \u003ca href="https://github.com/kubedl-io/website"\u003ehttps://github.com/kubedl-io/website\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eEvery action to make KubeDL better is encouraged.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eIf you find a typo, try to fix it!\u003c/li\u003e\n\u003cli\u003eIf you find a bug, try to fix it!\u003c/li\u003e\n\u003cli\u003eIf you find some redundant codes, try to remove them!\u003c/li\u003e\n\u003cli\u003eIf you find some test cases missing, try to add them!\u003c/li\u003e\n\u003cli\u003eIf you could enhance a feature, please DO NOT hesitate!\u003c/li\u003e\n\u003cli\u003eIf you find code implicit, try to add comments to make it clear!\u003c/li\u003e\n\u003cli\u003eIf you find code ugly, try to refactor that!\u003c/li\u003e\n\u003cli\u003eIf you can help to improve documents, it could not be better!\u003c/li\u003e\n\u003cli\u003eIf you find document incorrect, just do it and fix that!\u003c/li\u003e\n\u003cli\u003e\u0026hellip;\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAlthough contributions via PR is an explicit way to help, we still call for any other ways.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ereply to other\u0026rsquo;s issues if you could;\u003c/li\u003e\n\u003cli\u003ehelp solve other user\u0026rsquo;s problems;\u003c/li\u003e\n\u003cli\u003ehelp review other\u0026rsquo;s PR design;\u003c/li\u003e\n\u003cli\u003ehelp review other\u0026rsquo;s codes in PR;\u003c/li\u003e\n\u003cli\u003ediscuss about KubeDL to make things clearer;\u003c/li\u003e\n\u003cli\u003eadvocate KubeDL technology beyond GitHub;\u003c/li\u003e\n\u003cli\u003ewrite blogs on KubeDL and so on.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIn a word, \u003cstrong\u003eANY HELP IS CONTRIBUTION\u003c/strong\u003e.\u003c/p\u003e\n\u003ch2 id="join-kubedl-as-a-maintainer"\u003eJoin KubeDL as a maintainer\u003c/h2\u003e\n\u003cp\u003eYou are welcome to join KubeDL maintainer team if you are willing to participate. Please contact one of us in the community.\u003c/p\u003e\n\u003ch3 id="some-requirements"\u003eSome Requirements\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eHave submitted multiple PRs to the community\u003c/li\u003e\n\u003cli\u003eBe active in the community, may including but not limited\n\u003cul\u003e\n\u003cli\u003eSubmitting or commenting on issues\u003c/li\u003e\n\u003cli\u003eContributing PRs to the community\u003c/li\u003e\n\u003cli\u003eReviewing PRs in the community\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n'},{id:12,href:"https://kubedl.io/docs/prologue/install-using-helm/",title:"Install Using Helm",description:"Install KubeDL using Helm",content:'\u003ch2 id="install-helm"\u003eInstall Helm\u003c/h2\u003e\n\u003cp\u003eHelm is a package manager for Kubernetes. You can install helm with command below on MacOS\u003c/p\u003e\n\u003cdiv class="doks-clipboard"\u003e\n  \u003cbutton class="btn-clipboard btn btn-link" data-clipboard-text="brew install helm"\u003e\u003cspan class="copy-status"\u003e\u003c/span\u003e\u003c/button\u003e\n\u003c/div\u003e\n\n\u003cpre\u003e\u003ccode class="language-bash"\u003ebrew install helm\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eCheck the \u003ca href="https://helm.sh/docs/intro/install/"\u003ehelm website\u003c/a\u003e for more details.\u003c/p\u003e\n\u003ch2 id="install-kubedl"\u003eInstall KubeDL\u003c/h2\u003e\n\u003cp\u003eFrom the root directory, run\u003c/p\u003e\n\u003cdiv class="doks-clipboard"\u003e\n  \u003cbutton class="btn-clipboard btn btn-link" data-clipboard-text="helm install kubedl ./helm/kubedl --create-namespace -n kubedl-system"\u003e\u003cspan class="copy-status"\u003e\u003c/span\u003e\u003c/button\u003e\n\u003c/div\u003e\n\n\u003cpre\u003e\u003ccode class="language-bash"\u003ehelm install kubedl ./helm/kubedl --create-namespace -n kubedl-system\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eYou can override default values defined in \u003ca href="https://github.com/alibaba/kubedl/blob/master/helm/kubedl/values.yaml"\u003evalues.yaml\u003c/a\u003e with \u003ccode\u003e--set\u003c/code\u003e flag.\nFor example, set the custom cpu/memory resource:\u003c/p\u003e\n\u003cdiv class="doks-clipboard"\u003e\n  \u003cbutton class="btn-clipboard btn btn-link" data-clipboard-text="helm install kubedl ./helm/kubedl --create-namespace -n kubedl-system --set resources.requests.cpu=1024m --set resources.requests.memory=2Gi"\u003e\u003cspan class="copy-status"\u003e\u003c/span\u003e\u003c/button\u003e\n\u003c/div\u003e\n\n\u003cpre\u003e\u003ccode class="language-bash"\u003ehelm install kubedl ./helm/kubedl --create-namespace -n kubedl-system  --set resources.requests.cpu=1024m --set resources.requests.memory=2Gi\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eHelm will install CRDs and KubeDL controller under \u003ccode\u003ekubedl-system\u003c/code\u003e namespace.\u003c/p\u003e\n\u003ch2 id="uninstall-kubedl"\u003eUninstall KubeDL\u003c/h2\u003e\n\u003cdiv class="doks-clipboard"\u003e\n  \u003cbutton class="btn-clipboard btn btn-link" data-clipboard-text="helm uninstall kubedl -n kubedl-system"\u003e\u003cspan class="copy-status"\u003e\u003c/span\u003e\u003c/button\u003e\n\u003c/div\u003e\n\n\u003cpre\u003e\u003ccode class="language-bash"\u003ehelm uninstall kubedl -n kubedl-system\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2 id="delete-all-kubedlio-crds"\u003eDelete all kubedl.io CRDs\u003c/h2\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003ekubectl get crd | grep kubedl.io | cut -d \' \' -f 1 | xargs kubectl delete crd\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2 id="enable-specific-job-kind"\u003eEnable specific job Kind\u003c/h2\u003e\n\u003cp\u003eKubeDL supports all kinds of jobs(tensorflow, pytorch etc.) in a single Kubernetes operator. You can selectively enable the kind of jobs to support.\nThere are three options:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eDefault option. Just install the job CRDs required. KubeDL will automatically enable the corresponding job controller.\u003c/li\u003e\n\u003cli\u003eSet env \u003ccode\u003eWORKLOADS_ENABLE\u003c/code\u003e in KubeDL container. The value is a list of job types to be enabled. For example, \u003ccode\u003eWORKLOADS_ENABLE=TFJob,PytorchJob\u003c/code\u003e means only Tensorflow and Pytorch Job are enabled.\u003c/li\u003e\n\u003cli\u003eSet startup flags \u003ccode\u003e--workloads\u003c/code\u003e in KubeDL container command args. The value is a list of job types to be enabled like \u003ccode\u003e--workloads TFJob,PytorchJob\u003c/code\u003e.\u003c/li\u003e\n\u003c/ol\u003e\n'},{id:13,href:"https://kubedl.io/docs/tutorial/",title:"Tutorial",description:"Tutorial",content:""},{id:14,href:"https://kubedl.io/docs/workloads/",title:"Workloads",description:"Workloads",content:""},{id:15,href:"https://kubedl.io/docs/contributing/code-of-conduct/",title:"Code of Conduct",description:"code of conduct",content:'\u003ch2 id="our-pledge"\u003eOur Pledge\u003c/h2\u003e\n\u003cp\u003eWe as members, contributors, and leaders pledge to make participation in our\ncommunity a harassment-free experience for everyone, regardless of age, body\nsize, visible or invisible disability, ethnicity, sex characteristics, gender\nidentity and expression, level of experience, education, socio-economic status,\nnationality, personal appearance, race, religion, or sexual identity\nand orientation.\u003c/p\u003e\n\u003cp\u003eWe pledge to act and interact in ways that contribute to an open, welcoming,\ndiverse, inclusive, and healthy community.\u003c/p\u003e\n\u003ch2 id="our-standards"\u003eOur Standards\u003c/h2\u003e\n\u003cp\u003eExamples of behavior that contributes to a positive environment for our\ncommunity include:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDemonstrating empathy and kindness toward other people\u003c/li\u003e\n\u003cli\u003eBeing respectful of differing opinions, viewpoints, and experiences\u003c/li\u003e\n\u003cli\u003eGiving and gracefully accepting constructive feedback\u003c/li\u003e\n\u003cli\u003eAccepting responsibility and apologizing to those affected by our mistakes,\nand learning from the experience\u003c/li\u003e\n\u003cli\u003eFocusing on what is best not just for us as individuals, but for the\noverall community\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eExamples of unacceptable behavior include:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe use of sexualized language or imagery, and sexual attention or\nadvances of any kind\u003c/li\u003e\n\u003cli\u003eTrolling, insulting or derogatory comments, and personal or political attacks\u003c/li\u003e\n\u003cli\u003ePublic or private harassment\u003c/li\u003e\n\u003cli\u003ePublishing others\' private information, such as a physical or email\naddress, without their explicit permission\u003c/li\u003e\n\u003cli\u003eOther conduct which could reasonably be considered inappropriate in a\nprofessional setting\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id="enforcement-responsibilities"\u003eEnforcement Responsibilities\u003c/h2\u003e\n\u003cp\u003eCommunity leaders are responsible for clarifying and enforcing our standards of\nacceptable behavior and will take appropriate and fair corrective action in\nresponse to any behavior that they deem inappropriate, threatening, offensive,\nor harmful.\u003c/p\u003e\n\u003cp\u003eCommunity leaders have the right and responsibility to remove, edit, or reject\ncomments, commits, code, wiki edits, issues, and other contributions that are\nnot aligned to this Code of Conduct, and will communicate reasons for moderation\ndecisions when appropriate.\u003c/p\u003e\n\u003ch2 id="scope"\u003eScope\u003c/h2\u003e\n\u003cp\u003eThis Code of Conduct applies within all community spaces, and also applies when\nan individual is officially representing the community in public spaces.\nExamples of representing our community include using an official e-mail address,\nposting via an official social media account, or acting as an appointed\nrepresentative at an online or offline event.\u003c/p\u003e\n\u003ch2 id="enforcement"\u003eEnforcement\u003c/h2\u003e\n\u003cp\u003eInstances of abusive, harassing, or otherwise unacceptable behavior may be\nreported to the community leaders responsible for enforcement at\n.\nAll complaints will be reviewed and investigated promptly and fairly.\u003c/p\u003e\n\u003cp\u003eAll community leaders are obligated to respect the privacy and security of the\nreporter of any incident.\u003c/p\u003e\n\u003ch2 id="enforcement-guidelines"\u003eEnforcement Guidelines\u003c/h2\u003e\n\u003cp\u003eCommunity leaders will follow these Community Impact Guidelines in determining\nthe consequences for any action they deem in violation of this Code of Conduct:\u003c/p\u003e\n\u003ch3 id="1-correction"\u003e1. Correction\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eCommunity Impact\u003c/strong\u003e: Use of inappropriate language or other behavior deemed\nunprofessional or unwelcome in the community.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eConsequence\u003c/strong\u003e: A private, written warning from community leaders, providing\nclarity around the nature of the violation and an explanation of why the\nbehavior was inappropriate. A public apology may be requested.\u003c/p\u003e\n\u003ch3 id="2-warning"\u003e2. Warning\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eCommunity Impact\u003c/strong\u003e: A violation through a single incident or series\nof actions.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eConsequence\u003c/strong\u003e: A warning with consequences for continued behavior. No\ninteraction with the people involved, including unsolicited interaction with\nthose enforcing the Code of Conduct, for a specified period of time. This\nincludes avoiding interactions in community spaces as well as external channels\nlike social media. Violating these terms may lead to a temporary or\npermanent ban.\u003c/p\u003e\n\u003ch3 id="3-temporary-ban"\u003e3. Temporary Ban\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eCommunity Impact\u003c/strong\u003e: A serious violation of community standards, including\nsustained inappropriate behavior.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eConsequence\u003c/strong\u003e: A temporary ban from any sort of interaction or public\ncommunication with the community for a specified period of time. No public or\nprivate interaction with the people involved, including unsolicited interaction\nwith those enforcing the Code of Conduct, is allowed during this period.\nViolating these terms may lead to a permanent ban.\u003c/p\u003e\n\u003ch3 id="4-permanent-ban"\u003e4. Permanent Ban\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eCommunity Impact\u003c/strong\u003e: Demonstrating a pattern of violation of community\nstandards, including sustained inappropriate behavior,  harassment of an\nindividual, or aggression toward or disparagement of classes of individuals.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eConsequence\u003c/strong\u003e: A permanent ban from any sort of public interaction within\nthe community.\u003c/p\u003e\n\u003ch2 id="attribution"\u003eAttribution\u003c/h2\u003e\n\u003cp\u003eThis Code of Conduct is adapted from the \u003ca href="https://www.contributor-covenant.org"\u003eContributor Covenant\u003c/a\u003e,\nversion 2.0, available at\nhttps://www.contributor-covenant.org/version/2/0/code_of_conduct.html.\u003c/p\u003e\n\u003cp\u003eCommunity Impact Guidelines were inspired by \u003ca href="https://github.com/mozilla/diversity"\u003eMozilla\u0026rsquo;s code of conduct\nenforcement ladder\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eFor answers to common questions about this code of conduct, see the FAQ at\nhttps://www.contributor-covenant.org/faq. Translations are available at\nhttps://www.contributor-covenant.org/translations.\u003c/p\u003e\n'},{id:16,href:"https://kubedl.io/docs/prologue/install-using-yaml/",title:"Install Using Yaml",description:"",content:'\u003ch2 id="install-crds"\u003eInstall CRDs\u003c/h2\u003e\n\u003cp\u003eFrom \u003ca href="https://github.com/alibaba/kubedl"\u003eproject root directory\u003c/a\u003e, run\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003ekubectl apply -f config/crd/bases/\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2 id="install-kubedl-controller"\u003eInstall KubeDL controller\u003c/h2\u003e\n\u003cp\u003eA single yaml file including everything: deployment, rbac etc.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003ekubectl apply -f https://raw.githubusercontent.com/kubedl-io/kubedl/master/config/manager/all_in_one.yaml\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eKubeDL controller is installed under \u003ccode\u003ekubedl-system\u003c/code\u003e namespace.\u003c/p\u003e\n\u003cp\u003eThe official KubeDL controller image is hosted under \u003ca href="https://hub.docker.com/r/kubedl/kubedl"\u003edocker hub\u003c/a\u003e.\u003c/p\u003e\n\u003ch2 id="uninstall-kubedl-controller"\u003eUninstall KubeDL controller\u003c/h2\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003ekubectl delete namespace kubedl-system\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2 id="delete-crds"\u003eDelete CRDs\u003c/h2\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003ekubectl get crd | grep kubedl.io | cut -d \' \' -f 1 | xargs kubectl delete crd\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2 id="enable-specific-job-kind"\u003eEnable specific job Kind\u003c/h2\u003e\n\u003cp\u003eKubeDL supports all kinds of jobs(tensorflow, pytorch etc.) in a single Kubernetes operator. You can selectively enable the kind of jobs to support.\nThere are three options:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eDefault option. Just install the job CRDs required. KubeDL will automatically enable the corresponding job controller.\u003c/li\u003e\n\u003cli\u003eSet env \u003ccode\u003eWORKLOADS_ENABLE\u003c/code\u003e in KubeDL container. The value is a list of job types to be enabled. For example, \u003ccode\u003eWORKLOADS_ENABLE=TFJob,PytorchJob\u003c/code\u003e means only Tensorflow and Pytorch Job are enabled.\u003c/li\u003e\n\u003cli\u003eSet startup flags \u003ccode\u003e--workloads\u003c/code\u003e in KubeDL container command args. The value is a list of job types to be enabled like \u003ccode\u003e--workloads TFJob,PytorchJob\u003c/code\u003e.\u003c/li\u003e\n\u003c/ol\u003e\n'},{id:17,href:"https://kubedl.io/docs/workloads/mars/",title:"Mars",description:"Running mars on Kubernetes",content:'\u003ch2 id="whats-mars"\u003eWhat\u0026rsquo;s Mars\u003c/h2\u003e\n\u003cp\u003e\u003ccode\u003eMars\u003c/code\u003e is a tensor-based unified framework for large-scale data computation which scales Numpy, Pandas and Scikit-learn,\nsee \u003ca href="https://github.com/mars-project/mars"\u003emars-repo\u003c/a\u003e for details. As a data computation framework, \u003ccode\u003emars\u003c/code\u003e is easy to\nscale out and can run across hundreds of machines simultaneously to accelerate large scale data tasks.\u003cbr\u003e\u003c/p\u003e\n\u003cp\u003eA distributed mars job includes 3 roles to collaborate with each other：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eWebService\u003c/strong\u003e: web-service accepts requests from end-users and forwards the whole tensor-graph to scheduler, it provides a dashboard for end users to track job status and submit tasks interactively.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eScheduler\u003c/strong\u003e: scheduler compiles and holds a global view of tensor-graph, it schedules \u0026lsquo;operands\u0026rsquo; and \u0026lsquo;chunks\u0026rsquo; to workers.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWorker\u003c/strong\u003e:  worker listen to \u0026lsquo;operands\u0026rsquo; and \u0026lsquo;chunks\u0026rsquo; dispatched by scheduler, executes the tasks, and reports results back to scheduler.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id="run-mars-with-kubedl"\u003eRun Mars with KubeDL\u003c/h2\u003e\n\u003cp\u003eRun \u003ccode\u003emars\u003c/code\u003e job on kubernetes natively.\u003c/p\u003e\n\u003ch3 id="1-deploy-kubedl"\u003e1. Deploy KubeDL\u003c/h3\u003e\n\u003cp\u003eFollow the \u003ca href="https://kubedl.io/docs/prologue/introduction/"\u003einstallation tutorial\u003c/a\u003e.\u003c/p\u003e\n\u003ch3 id="2-apply-mars-crd"\u003e2. Apply Mars CRD\u003c/h3\u003e\n\u003cp\u003e\u003ccode\u003eMars\u003c/code\u003e CRD(CustomResourceDefinition) manifest file describes the structure of a mars job spec. Run the following to apply the CRD:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003ekubectl apply -f https://raw.githubusercontent.com/alibaba/kubedl/v0.3.0/config/crd/bases/training.kubedl.io_marsjobs.yaml\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3 id="3-create-a-mars-job"\u003e3. Create a Mars Job\u003c/h3\u003e\n\u003cp\u003eCreate a YAML spec that describes the requirements of a MarsJob such as the worker, scheduler, WebService like below\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-yaml"\u003eapiVersion: training.kubedl.io/v1alpha1\nkind: MarsJob\nmetadata:\n  name: mars-test-demo\n  namespace: default\nspec:\n  cleanPodPolicy: None\n  webHost: mars.domain.com\n  marsReplicaSpecs:\n    Scheduler:\n      replicas: 1\n      restartPolicy: Never\n      template:\n        metadata:\n          labels:\n            mars/service-type: marsscheduler\n        spec:\n          containers:\n            - command:\n                - /bin/sh\n                - -c\n                - python -m mars.deploy.kubernetes.scheduler\n              image: mars-image\n              imagePullPolicy: Always\n              name: mars\n              resources:\n                limits:\n                  cpu: 2\n                  memory: 2Gi\n                requests:\n                  cpu: 2\n                  memory: 2Gi\n          serviceAccountName: kubedl-sa\n    WebService:\n      replicas: 1\n      restartPolicy: Never\n      template:\n        metadata:\n          labels:\n            mars/service-type: marswebservice\n        spec:\n          containers:\n            - command:\n                - /bin/sh\n                - -c\n                - python -m mars.deploy.kubernetes.web\n              image: mars-image\n              imagePullPolicy: Always\n              name: mars\n              resources:\n                limits:\n                  cpu: 2\n                  memory: 2Gi\n                requests:\n                  cpu: 2\n                  memory: 2Gi\n          serviceAccountName: kubedl-sa\n    Worker:\n      replicas: 2\n      restartPolicy: Never\n      template:\n        metadata:\n          labels:\n            mars/service-type: marsworker\n        spec:\n          containers:\n            - command:\n                - /bin/sh\n                - -c\n                - python -m mars.deploy.kubernetes.worker\n              image: mars-image\n              imagePullPolicy: Always\n              name: mars\n              resources:\n                limits:\n                  cpu: 2\n                  memory: 2Gi\n                requests:\n                  cpu: 2\n                  memory: 2Gi\n          serviceAccountName: kubedl-sa\nstatus: {}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe \u003ccode\u003espec\u003c/code\u003e field describes the requirement of each replica, including \u003ccode\u003ereplicas\u003c/code\u003e, \u003ccode\u003erestartPolicy\u003c/code\u003e, \u003ccode\u003etemplate\u003c/code\u003e\u0026hellip;and\nthe \u003ccode\u003estatus\u003c/code\u003e field describes the job current status. Run following command to start an example mars job:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003ekubectl create -f example/mars/mars-test-demo.yaml\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eCheck the mars job status:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003e$ kubectl get marsjob\nNAME             STATE     AGE   FINISHED-TTL   MAX-LIFETIME\nmars-test-demo   Running   40m\n$ kubectl get pods\nNAME                                            READY   STATUS             RESTARTS   AGE\nmars-test-demo-scheduler-0                      1/1     Running            0          40m\nmars-test-demo-webservice-0                     1/1     Running            0          40m\nmars-test-demo-worker-0                         1/1     Running            0          40m\nmars-test-demo-worker-1                         1/1     Running            0          40m\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3 id="4-access-web-service"\u003e4. Access web-service.\u003c/h3\u003e\n\u003cfigure\u003e\n  \u003cimg class="img-fluid lazyload blur-up" data-sizes="auto" src="https://kubedl.io/docs/workloads/mars/mars-ingress_hu3d024fe99bd4d562ba755cd6f9fa87db_47305_20x0_resize_box_2.png" data-srcset="https://kubedl.io/docs/workloads/mars/mars-ingress_hu3d024fe99bd4d562ba755cd6f9fa87db_47305_900x0_resize_box_2.png 900w,https://kubedl.io/docs/workloads/mars/mars-ingress_hu3d024fe99bd4d562ba755cd6f9fa87db_47305_800x0_resize_box_2.png 800w,https://kubedl.io/docs/workloads/mars/mars-ingress_hu3d024fe99bd4d562ba755cd6f9fa87db_47305_700x0_resize_box_2.png 700w,https://kubedl.io/docs/workloads/mars/mars-ingress_hu3d024fe99bd4d562ba755cd6f9fa87db_47305_600x0_resize_box_2.png 600w,https://kubedl.io/docs/workloads/mars/mars-ingress_hu3d024fe99bd4d562ba755cd6f9fa87db_47305_500x0_resize_box_2.png 500w" width="892" height="452" alt="mars-ingress"\u003e\n  \u003cnoscript\u003e\u003cimg class="img-fluid" sizes="100vw" srcset="https://kubedl.io/docs/workloads/mars/mars-ingress_hu3d024fe99bd4d562ba755cd6f9fa87db_47305_900x0_resize_box_2.png 900w,https://kubedl.io/docs/workloads/mars/mars-ingress_hu3d024fe99bd4d562ba755cd6f9fa87db_47305_800x0_resize_box_2.png 800w,https://kubedl.io/docs/workloads/mars/mars-ingress_hu3d024fe99bd4d562ba755cd6f9fa87db_47305_700x0_resize_box_2.png 700w,https://kubedl.io/docs/workloads/mars/mars-ingress_hu3d024fe99bd4d562ba755cd6f9fa87db_47305_600x0_resize_box_2.png 600w,https://kubedl.io/docs/workloads/mars/mars-ingress_hu3d024fe99bd4d562ba755cd6f9fa87db_47305_500x0_resize_box_2.png 500w" src="https://kubedl.io/docs/workloads/mars/mars-ingress.png" width="892" height="452" alt="mars-ingress"\u003e\u003c/noscript\u003e\n  \u003cfigcaption class="figure-caption"\u003emars-ingress\u003c/figcaption\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eWeb service visualizes job status, computation process progress and provides an entry for interactive submission.\nHowever, web service instance was running as a pod inside a kubernetes cluster which may not be accessible by external users.\n\u003ccode\u003eKubeDL\u003c/code\u003e provides two access modes for users in different network environment.\u003c/p\u003e\n\u003ch4 id="41-access-web-service-in-cluster"\u003e4.1 Access web-service in-cluster.\u003c/h4\u003e\n\u003cp\u003eFor users in the same network environment with web service instance, they can directly access its \u003cem\u003eservice\u003c/em\u003e without any other additional configurations,\nand the address is formatted as: \u003ccode\u003e{webservice-name}.{namespace}\u003c/code\u003e, it is a \u003ccode\u003eA\u003c/code\u003e record generated by \u003ccode\u003eCoreDNS\u003c/code\u003e, so you have to ensure that \u003ccode\u003eCoreDNS\u003c/code\u003e has been\ndeployed.\u003c/p\u003e\n\u003ch4 id="42-access-web-service-outside-cluster"\u003e4.2 Access web-service outside cluster.\u003c/h4\u003e\n\u003cp\u003eFor users in different network environment(e.g. an internet user wants to access a mars web-service running in vpc),\nusers have to apply an SLB address first, so that they can ping the ip in \u003cstrong\u003evpc\u003c/strong\u003e with a public address by SLB domain resolving, then in job spec, users just need fill the \u003ccode\u003espec.webHost\u003c/code\u003e field with\ntheir applied SLB address, \u003ccode\u003eKubeDL\u003c/code\u003ewill generated ingress instance with routing rules, so that external traffic can be routed to target web service and that\nbecomes available for outside users.\u003c/p\u003e\n\u003ch3 id="5-memory-tuning-policy"\u003e5. Memory Tuning Policy\u003c/h3\u003e\n\u003cp\u003e\u003ccode\u003eWorker\u003c/code\u003e is the role that actually performs computing tasks in \u003ccode\u003eMarsJob\u003c/code\u003e.\nMars supports running jobs in different memory usage scenarios. For example, swap cold in-memory data out to spill dirs and persist in kubernetes ephemeral-storage.\n\u003ccode\u003eMars\u003c/code\u003e provides plentiful memory tuning options which has been integrated to \u003ccode\u003eMarsJob\u003c/code\u003e type definition, including :\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eplasmaStore: PlasmaStore specify the socket path of plasma store that handles shared memory between all worker processes.\u003c/li\u003e\n\u003cli\u003elockFreeFileIO: LockFreeFileIO indicates whether spill dirs are dedicated or not.\u003c/li\u003e\n\u003cli\u003espillDirs: SpillDirs specify multiple directory paths, when size of in-memory objects is about to reach the limitation, mars workers will swap cold data out to spill dirs and persist in ephemeral-storage.\u003c/li\u003e\n\u003cli\u003eworkerCachePercentage: WorkerCachePercentage specify the percentage of total available memory size can be used as cache, it will be overridden by workerCacheSize if it is been set.\u003c/li\u003e\n\u003cli\u003eworkerCacheSize：WorkerCacheSize specify the exact cache quantity can be used.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eusers can set above options in \u003ccode\u003ejob.spec.memoryTuningPolicy\u003c/code\u003e field:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-yaml"\u003eapiVersion: training.kubedl.io/v1alpha1\nkind: MarsJob\nmetadata:\n  name: mars-test-demo\n  namespace: default\nspec:\n  cleanPodPolicy: None\n  memoryTuningPolicy:\n    plasmaStore: string              # /etc/pstore/...\n    lockFreeFileIO: bool             # false\n    spillDirs: []string              # ...\n    workerCachePercentage: int32     # 80, indicates 80%\n    workerCacheSize: quantity        # 10Gi\n  marsReplicaSpecs:\n    ...\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2 id="run-mars-in-standalone-mode"\u003eRun Mars in Standalone Mode\u003c/h2\u003e\n\u003cp\u003eIn standalone mode, a distributed \u003ccode\u003eMars\u003c/code\u003e job are running standalone on bare hosts without the help of other container orchestration tools.\nBut this requires manual configuration effort and lack other abilities such as automatic failover of workers.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003erun \u003ccode\u003epip install pymars[distributed]\u003c/code\u003e on every node in the cluster to install dependencies needed for distributed execution.\u003c/li\u003e\n\u003cli\u003estart different mars role processes on each node.\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003emars-scheduler -a \u0026lt;scheduler_ip\u0026gt; -p \u0026lt;scheduler_port\u0026gt;\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003emars-web -a \u0026lt;web_ip\u0026gt; -p \u0026lt;web_port\u0026gt; -s \u0026lt;scheduler_ip\u0026gt;:\u0026lt;scheduler_port\u0026gt;\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003emars-worker -a \u0026lt;worker_ip\u0026gt; -p \u0026lt;worker_port\u0026gt; -s \u0026lt;scheduler_ip\u0026gt;:\u0026lt;scheduler_port\u0026gt;\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eusually there must be at least 1 web-service and 1 scheduler and a certain number of workers.\u003c/li\u003e\n\u003cli\u003eafter all processes started, users can open the python console run snippet to create a session with web-service and submit tasks.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode class="language-python"\u003eimport mars.tensor as mt\nimport mars.dataframe as md\nfrom mars.session import new_session\nnew_session(\'http://\u0026lt;web_ip\u0026gt;:\u0026lt;web_port\u0026gt;\').as_default()\na = mt.ones((2000, 2000), chunk_size=200)\nb = mt.inner(a, a)\nb.execute()  # submit tensor to cluster\ndf = md.DataFrame(a).sum()\ndf.execute()  # submit DataFrame to cluster\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2 id="crd-spec"\u003eCRD Spec\u003c/h2\u003e\n\u003cp\u003eCheck the CRD definition. \u003ca href="https://github.com/alibaba/kubedl/blob/master/apis/training/v1alpha1/marsjob_types.go"\u003eGo -\u0026gt;\u003c/a\u003e\u003c/p\u003e\n'},{id:18,href:"https://kubedl.io/docs/recipes/",title:"Recipes",description:"Recipes",content:""},{id:19,href:"https://kubedl.io/docs/help/dingtalk/",title:"Dingtalk",description:"Dingtalk Help.",content:'\u003cp\u003eGet help on joining the Dingtalk(钉钉) Group\u003c/p\u003e\n\u003cfigure\u003e\n  \u003cimg class="img-fluid lazyload blur-up" data-sizes="auto" src="https://kubedl.io/docs/help/dingtalk/kubedl_hu0d572a5a19c12dcb10b88d4fb51252c9_212191_20x0_resize_box_2.png" data-srcset="https://kubedl.io/docs/help/dingtalk/kubedl_hu0d572a5a19c12dcb10b88d4fb51252c9_212191_800x0_resize_box_2.png 800w,https://kubedl.io/docs/help/dingtalk/kubedl_hu0d572a5a19c12dcb10b88d4fb51252c9_212191_700x0_resize_box_2.png 700w,https://kubedl.io/docs/help/dingtalk/kubedl_hu0d572a5a19c12dcb10b88d4fb51252c9_212191_600x0_resize_box_2.png 600w,https://kubedl.io/docs/help/dingtalk/kubedl_hu0d572a5a19c12dcb10b88d4fb51252c9_212191_500x0_resize_box_2.png 500w" width="828" height="1068" alt="Dingtalk"\u003e\n  \u003cnoscript\u003e\u003cimg class="img-fluid" sizes="100vw" srcset="https://kubedl.io/docs/help/dingtalk/kubedl_hu0d572a5a19c12dcb10b88d4fb51252c9_212191_800x0_resize_box_2.png 800w,https://kubedl.io/docs/help/dingtalk/kubedl_hu0d572a5a19c12dcb10b88d4fb51252c9_212191_700x0_resize_box_2.png 700w,https://kubedl.io/docs/help/dingtalk/kubedl_hu0d572a5a19c12dcb10b88d4fb51252c9_212191_600x0_resize_box_2.png 600w,https://kubedl.io/docs/help/dingtalk/kubedl_hu0d572a5a19c12dcb10b88d4fb51252c9_212191_500x0_resize_box_2.png 500w" src="https://kubedl.io/docs/help/dingtalk/kubedl.png" width="828" height="1068" alt="Dingtalk"\u003e\u003c/noscript\u003e\n  \u003cfigcaption class="figure-caption"\u003eDingtalk\u003c/figcaption\u003e\n\u003c/figure\u003e\n\n'},{id:20,href:"https://kubedl.io/docs/recipes/tensorboard/",title:"Tensorboard",description:"",content:'\u003cp\u003eKubeDL can attach a tensorboard to a running tensorflow job.\nUsers can visualize the tensorflow job with the tensorboard.\u003c/p\u003e\n\u003cp\u003eTo use tensorboard, users must ensure that the tensorflow job logs are created and stored in a kubernetes remote volume (emptyDir, hostPath and local volume are not supported) and the tensorboard pod can mount the volume.\u003c/p\u003e\n\u003cp\u003eUsers can set the tensorboard config in the job\u0026rsquo;s annotation with key \u003ccode\u003ekubedl.io/tensorboard-config\u003c/code\u003e as below.\nAfter that, users can access the tensorboard through this URL \u003ccode\u003ehttp://\u0026lt;ingress host\u0026gt;/\u0026lt;ingress pathPrefix\u0026gt;/\u0026lt;job namespace\u0026gt;/\u0026lt;job name\u0026gt;\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eFor example:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-yaml"\u003e    apiVersion: \u0026quot;training.kubedl.io/v1alpha1\u0026quot;\n    kind: \u0026quot;TFJob\u0026quot;\n    metadata:\n      name: \u0026quot;mnist\u0026quot;\n      namespace: kubedl\n      annotations:\n +      kubedl.io/tensorboard-config: \'{\u0026quot;logDir\u0026quot;:\u0026quot;/var/log/training\u0026quot;,\u0026quot;ttlSecondsAfterJobFinished\u0026quot;:3600,\u0026quot;ingressSpec\u0026quot;:{\u0026quot;host\u0026quot;:\u0026quot;locahost\u0026quot;,\u0026quot;pathPrefix\u0026quot;:\u0026quot;/tb\u0026quot;}}\'\n    spec:\n      cleanPodPolicy: None\n      tfReplicaSpecs:\n        ...\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eA full list of supported options are:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-json5"\u003e{\n    \u0026quot;logDir\u0026quot;: \u0026quot;xxx\u0026quot;,            // the path of the tensorflow job logs (required).\n    \u0026quot;ttlSecondsAfterJobFinished\u0026quot;: 3600,     // the TTL to clean up the tensorboard after the job is finished (required).\n    \u0026quot;image\u0026quot;: \u0026quot;xxx\u0026quot;,             // the image of the tensorboard, default value is the job\'s image (optional).\n    \u0026quot;ingressSpec\u0026quot;: {            // the ingress of the tensorboard (required).\n        \u0026quot;host\u0026quot;: \u0026quot;xxx\u0026quot;,          // the ingress host (required).\n        \u0026quot;pathPrefix\u0026quot;: \u0026quot;xxx\u0026quot;,    // the pathPrefix will be set to the ingress path with the pattern: \u0026lt;pathPrefix\u0026gt;/\u0026lt;job namespace\u0026gt;/\u0026lt;job name\u0026gt; (required).\n        \u0026quot;annotations\u0026quot;: {        // the annotations of the ingress (optional).\n            \u0026quot;xxx\u0026quot;: \u0026quot;xxx\u0026quot;\n        }\n    }\n}\n\u003c/code\u003e\u003c/pre\u003e\n'},{id:21,href:"https://kubedl.io/docs/contributing/comparisons/",title:"Comparisons",description:"Comparisons",content:'\u003ch2 id="comparisons-with-kubeflow"\u003eComparisons with Kubeflow\u003c/h2\u003e\n\u003cp\u003eKubeDL supports training, modeling lineage versioning, inference, and automatic tuning for model deployment on Kubernetes.\nKubeflow is a machine learning toolkit for Kubernetes that contains several projects for different purposes.\u003c/p\u003e\n\u003cp\u003eBoth projects share the same goal to enable machine learning run on Kubernetes, but largely solve different problems,\nor in different approaches, or can complement each other in this area.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eKubeDL modeling lineage and versioning: KubeDL tracks the history of a model in the native Kubernetes CRD format which\nis unique and easy for existing Kubernetes users to adopt and can leverage broad Kubernetes tooling ecosystem.\nIt is integrated natively with the training and serving CRD to enable the full automation with controller.\nThat is, from the training job, to the model generated, to the deployment of the model all are linearly tracked and versioned\nwithin Kubernetes. KubeDL model can supposedly work together with Kubeflow as well with some code changes in certain operators,\nto enable model lineage and versioning in Kuberentes.\u003c/li\u003e\n\u003cli\u003eKubeDL auto tuning for model deployment tries to figure out the best configurations for inference container with respect to\nits container resources or running parameters . It leverages data and machine learning models to find the optimal\nconfiguration set for its performance or container resource demand, or balance the tradeoff between them. The\narchitecture is implemented in standard Kubernetes CRD with controller and can be easily adopted by existing Kubernetes users, whereas this functionality does not exist in Kubeflow\u003c/li\u003e\n\u003cli\u003eKubeDL training shares some functionalities with certain Kubeflow training operators to enable training jobs run on Kubernetes.\nBut there are noticeable feature differences in KubeDL from in Kubeflow:\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eKubeDL enables more machine learning frameworks particularly the two open sourced projects by Alibaba mentioned below,\nin addition to industry trending frameworks(TensorFlow, PyTorch, MPI) in a single controller.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ehttps://github.com/mars-project/mars  Mars is a tensor-based unified framework for large-scale data computation\nwhich scales Numpy, pandas, Scikit-learn and Python functions. Think of it as the distributed version of numpy, but there are more than that.\u003c/li\u003e\n\u003cli\u003ehttps://github.com/alibaba/x-deeplearning An industrial deep learning framework for high-dimension sparse data.\nThis project stems form Alibaba internal large scale deep learning production practices targeted for high dimensional sparse data typically found in e-commerce.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eIntegrate multi framework support in a single controller so that coherent metrics and monitoring support is possible,\nand that reduces operational burdens and resource allocations (compared with per framework per controller in Kubeflow.)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eBuilt-in metadata and events persistent controller to save job metadata and events in external storages\nsuch as MySQL to outlive api-server state. This functionality does not exist in Kubeflow and can potentially complement that in Kubeflow.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eSchedule jobs in a cron-like way.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eEnable jobs run in host network mode in heterogeneous cluster environments. Due to large complexity in Alibaba and Alibaba\u0026rsquo;s customer\u0026rsquo;s environments,\nhost network sometimes becomes a must-have feature to enable machine learning job run on Kubernetes.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eAutomatic file sync into container to include external artifacts on job launch.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eBuilt-in advanced scheduling strategy practiced in Alibaba internal world\u0026rsquo;s largest GPU and CPU clusters.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eDag scheduling strategy across different roles in distributed machine learning setting.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eAnd more\u0026hellip;\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThat said, some features can be enabled by running KubeDL as-is, to complement same missing functionalities in Kubeflow.\u003c/p\u003e\n'},{id:22,href:"https://kubedl.io/docs/prologue/quick-start/",title:"Quick Start",description:"Run a simple MNist Tensorflow job with KubeDL.",content:'\u003ch2 id="submit-the-tensorflow-job"\u003eSubmit the TensorFlow job\u003c/h2\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003ekubectl apply -f https://raw.githubusercontent.com/alibaba/kubedl/v0.3.0/example/tf/tf_job_mnist.yaml\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3 id="get-job-status"\u003eGet job status\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003ekubectl get tfjobs -n kubedl\nkubectl describe tfjob mnist -n kubedl\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3 id="delete-the-job"\u003eDelete the job\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003ekubectl delete tfjob mnist -n kubedl\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2 id="other-commands"\u003eOther commands\u003c/h2\u003e\n\u003cp\u003e\u003ca href="https://kubedl.io/docs/prologue/commands/"\u003eCommands →\u003c/a\u003e\u003c/p\u003e\n'},{id:23,href:"https://kubedl.io/docs/references/",title:"References",description:"References",content:""},{id:24,href:"https://kubedl.io/docs/contributing/",title:"Contributing",description:"Contributing",content:""},{id:25,href:"https://kubedl.io/docs/recipes/metadata-persistency/",title:"Metadata Persistency",description:"",content:'\u003cp\u003eKubernetes api-server typically stores job information for a limited lifespan. KubeDL has built-in support to persist the\njob metadata into external storage to outlive api-server state.\nThe KubeDL controller will persist the job metadata during the lifecycle of job such as job and pod creation/deletion.\u003c/p\u003e\n\u003cp\u003eCurrently, only \u003ccode\u003eMysql\u003c/code\u003e is supported.\u003c/p\u003e\n\u003ch2 id="db-schema"\u003eDB Schema\u003c/h2\u003e\n\u003ch3 id="job-table"\u003eJob Table\u003c/h3\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e\u003cstrong\u003eColumn\u003c/strong\u003e\u003c/th\u003e\n\u003cth\u003e\u003cstrong\u003eType\u003c/strong\u003e\u003c/th\u003e\n\u003cth\u003e\u003cstrong\u003eDescription\u003c/strong\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003eid\u003c/td\u003e\n\u003ctd\u003eint(64)\u003c/td\u003e\n\u003ctd\u003eprimary id auto incremented by underlying database\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003ename\u003c/td\u003e\n\u003ctd\u003evarchar(128)\u003c/td\u003e\n\u003ctd\u003ename of job\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003enamespace\u003c/td\u003e\n\u003ctd\u003evarchar(128)\u003c/td\u003e\n\u003ctd\u003enamespace of job\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003ejob_id\u003c/td\u003e\n\u003ctd\u003evarchar(64)\u003c/td\u003e\n\u003ctd\u003ejob uid generated by kubernetes\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eversion\u003c/td\u003e\n\u003ctd\u003evarchar(32)\u003c/td\u003e\n\u003ctd\u003eresource version generated by kubernetes(etcd)\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003estatus\u003c/td\u003e\n\u003ctd\u003evarchar(32)\u003c/td\u003e\n\u003ctd\u003ecurrent observed job status(Created/Running/Failed/Succeed/Restarting)\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003ekind\u003c/td\u003e\n\u003ctd\u003evarchar(32)\u003c/td\u003e\n\u003ctd\u003ekind of job: TFJob,PyTorchJob\u0026hellip;\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eresources\u003c/td\u003e\n\u003ctd\u003etext\u003c/td\u003e\n\u003ctd\u003ejob requested resources, including replicas and resources of each role.\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003edeploy_region\u003c/td\u003e\n\u003ctd\u003evarchar(64)\u003c/td\u003e\n\u003ctd\u003edeploy_region indicates the physical region(IDC) this job located in, reserved for jobs running in across-region-clusters\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003etenant\u003c/td\u003e\n\u003ctd\u003evarchar(255)\u003c/td\u003e\n\u003ctd\u003efields reserved for multi-tenancy job management scenarios, indicating which tenant this job belongs to and who\u0026rsquo;s the owner(user)\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eowner\u003c/td\u003e\n\u003ctd\u003evarchar(255)\u003c/td\u003e\n\u003ctd\u003eowner of job\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eis_in_etcd\u003c/td\u003e\n\u003ctd\u003etinyint(4)\u003c/td\u003e\n\u003ctd\u003eis_in_etcd indicates that whether record of this job has been removed from etcd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003egmt_created\u003c/td\u003e\n\u003ctd\u003edatetime\u003c/td\u003e\n\u003ctd\u003etimestamp when job created\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003egmt_modified\u003c/td\u003e\n\u003ctd\u003edatetime\u003c/td\u003e\n\u003ctd\u003etimestamp when last job status transited\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003egmt_finished\u003c/td\u003e\n\u003ctd\u003edatetime\u003c/td\u003e\n\u003ctd\u003etimestamp when job failed or succeed\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id="pod-table"\u003ePod Table\u003c/h3\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e\u003cstrong\u003eColumn\u003c/strong\u003e\u003c/th\u003e\n\u003cth\u003e\u003cstrong\u003eType\u003c/strong\u003e\u003c/th\u003e\n\u003cth\u003e\u003cstrong\u003eDescription\u003c/strong\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003eid\u003c/td\u003e\n\u003ctd\u003eint(64)\u003c/td\u003e\n\u003ctd\u003eprimary id auto incremented by underlying database\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003ename\u003c/td\u003e\n\u003ctd\u003evarchar(128)\u003c/td\u003e\n\u003ctd\u003ename of pod\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003enamespace\u003c/td\u003e\n\u003ctd\u003evarchar(128)\u003c/td\u003e\n\u003ctd\u003enamespace of pod\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003epod_id\u003c/td\u003e\n\u003ctd\u003evarchar(64)\u003c/td\u003e\n\u003ctd\u003epod uid generated by kubernetes\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eversion\u003c/td\u003e\n\u003ctd\u003evarchar(32)\u003c/td\u003e\n\u003ctd\u003eresource version generated by kubernetes(etcd)\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003estatus\u003c/td\u003e\n\u003ctd\u003evarchar(32)\u003c/td\u003e\n\u003ctd\u003ecurrent observed pod phase(Pending/Running/Failed/Succeed/Unkown)\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eimage\u003c/td\u003e\n\u003ctd\u003evarchar(255)\u003c/td\u003e\n\u003ctd\u003eimage name of main conatiner\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003ejob_id\u003c/td\u003e\n\u003ctd\u003evarchar(64)\u003c/td\u003e\n\u003ctd\u003ejob id of this pod controlled by\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003ereplica_type\u003c/td\u003e\n\u003ctd\u003evarchar(32)\u003c/td\u003e\n\u003ctd\u003ereplica type of this pod belongs to\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eresources\u003c/td\u003e\n\u003ctd\u003evarchar(1024)\u003c/td\u003e\n\u003ctd\u003eresources this pod requested, marshaled from a ResourceRequirements object\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003ehost_ip\u003c/td\u003e\n\u003ctd\u003evarchar(64)\u003c/td\u003e\n\u003ctd\u003eip of the host this pod scheduled\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003epod_ip\u003c/td\u003e\n\u003ctd\u003evarchar(64)\u003c/td\u003e\n\u003ctd\u003eip of this pod allocated\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003edeploy_region\u003c/td\u003e\n\u003ctd\u003evarchar(64)\u003c/td\u003e\n\u003ctd\u003edeploy_region indicates the physical region(IDC) this job located in\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eis_in_etcd\u003c/td\u003e\n\u003ctd\u003etinyint(4)\u003c/td\u003e\n\u003ctd\u003eis_in_etcd indicates that whether record of this job has been removed from etcd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eremark\u003c/td\u003e\n\u003ctd\u003etext\u003c/td\u003e\n\u003ctd\u003eextended messaged for pod\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003egmt_created\u003c/td\u003e\n\u003ctd\u003edatetime\u003c/td\u003e\n\u003ctd\u003etimestamp when pod created\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003egmt_modified\u003c/td\u003e\n\u003ctd\u003edatetime\u003c/td\u003e\n\u003ctd\u003etimestamp when last pod status transited\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003egmt_started\u003c/td\u003e\n\u003ctd\u003edatetime\u003c/td\u003e\n\u003ctd\u003etimestamp when main container stared\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003egmt_finished\u003c/td\u003e\n\u003ctd\u003edatetime\u003c/td\u003e\n\u003ctd\u003etimestamp when pod failed or succeed\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch2 id="how-to-use"\u003eHow To Use\u003c/h2\u003e\n\u003cp\u003eBelow is an example to setup KubeDL to use \u003ccode\u003eMysql\u003c/code\u003e as the persistency DB.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eSet up credentials for KubeDL to connect to DB. Create a \u003ccode\u003eSecret\u003c/code\u003e object like below:\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode class="language-yaml"\u003eapiVersion: v1\nkind: Secret\nmetadata:\n  name: kubedl-mysql-config\n  namespace: kubedl-system\ntype: Opaque\nstringData:\n  host: my.host.com\n  dbName: kubedl\n  user: kubedl-user\n  password: this-is-me\n  port: \u0026quot;3306\u0026quot;\n\u003c/code\u003e\u003c/pre\u003e\n\u003col start="2"\u003e\n\u003cli\u003eUpdate the Kubedl Deployment spec to include \u003ccode\u003e--meta-storage mysql\u003c/code\u003e in the startup flag and reference the DB credentials\nvia environment variables. The KubeDL controller uses the env to set up connection with DB.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode class="language-yaml"\u003eapiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kubedl\n  namespace: kubedl-system\n  labels:\n    app: kubedl\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: kubedl\n  template:\n    metadata:\n      labels:\n        app: kubedl\n    spec:\n      containers:\n      - image: kubedl/kubedl:v0.3.0\n        imagePullPolicy: Always\n        name: kubedl-manager\n        args:\n        - \u0026quot;--meta-storage\u0026quot;\n        - \u0026quot;mysql\u0026quot;\n        env:\n        - name: MYSQL_HOST\n          value:\n          valueFrom:\n            secretKeyRef:\n              name: kubedl-mysql-config\n              key: host\n        - name: MYSQL_DB_NAME\n          value:\n          valueFrom:\n            secretKeyRef:\n              name: kubedl-mysql-config\n              key: dbName\n        - name: MYSQL_USER\n          value:\n          valueFrom:\n            secretKeyRef:\n              name: kubedl-mysql-config\n              key: user\n        - name: MYSQL_PASSWORD\n          value:\n          valueFrom:\n            secretKeyRef:\n              name: kubedl-mysql-config\n              key: password\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2 id="mysql-config"\u003eMySql Config\u003c/h2\u003e\n\u003cp\u003eThe configs defined in the aforementioned \u003ccode\u003esecret\u003c/code\u003e:\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eConfig Name\u003c/th\u003e\n\u003cth\u003eDescription\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003ehost\u003c/td\u003e\n\u003ctd\u003eMysql host name\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003edbName\u003c/td\u003e\n\u003ctd\u003eDB name\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003euser\u003c/td\u003e\n\u003ctd\u003eUser name\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003epassword\u003c/td\u003e\n\u003ctd\u003eUser password\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eport\u003c/td\u003e\n\u003ctd\u003eThe mysql DB port to connect to\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch2 id="contributions"\u003eContributions\u003c/h2\u003e\n\u003cp\u003eCurrently, only \u003ccode\u003emysql\u003c/code\u003e is supported. You are welcome to contribute your own storage plugin.\u003c/p\u003e\n'},{id:26,href:"https://kubedl.io/docs/workloads/xgboost/",title:"XGBoost",description:"Run XGBoost on Kubernetes.",content:'\u003ch2 id="example"\u003eExample\u003c/h2\u003e\n\u003cpre\u003e\u003ccode class="language-yaml"\u003eapiVersion: training.kubedl.io/v1alpha1\nkind: \u0026quot;XGBoostJob\u0026quot;\nmetadata:\n  name: \u0026quot;xgboost-dist-iris-test-train\u0026quot;\nspec:\n  xgbReplicaSpecs:\n    Master:\n      replicas: 1\n      restartPolicy: Never\n      template:\n        apiVersion: v1\n        kind: Pod\n        spec:\n          containers:\n            - name: xgboostjob\n              image: docker.io/merlintang/xgboost-dist-iris:1.1\n              ports:\n                - containerPort: 9991\n                  name: xgboostjob-port\n              imagePullPolicy: Always\n              args:\n                - --job_type=Train\n                - --xgboost_parameter=objective:multi:softprob,num_class:3\n                - --n_estimators=10\n                - --learning_rate=0.1\n                - --model_path=autoAI/xgb-opt/2\n                - --model_storage_type=oss\n                - --oss_param=unknown\n    Worker:\n      replicas: 2\n      restartPolicy: ExitCode\n      template:\n        apiVersion: v1\n        kind: Pod\n        spec:\n          containers:\n            - name: xgboostjob\n              image: docker.io/merlintang/xgboost-dist-iris:1.1\n              ports:\n                - containerPort: 9991\n                  name: xgboostjob-port\n              imagePullPolicy: Always\n              args:\n                - --job_type=Train\n                - --xgboost_parameter=\u0026quot;objective:multi:softprob,num_class:3\u0026quot;\n                - --n_estimators=10\n                - --learning_rate=0.1\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2 id="crd-spec"\u003eCRD Spec\u003c/h2\u003e\n\u003cp\u003eCheck the CRD definition. \u003ca href="https://github.com/alibaba/kubedl/blob/master/apis/training/v1alpha1/xgboostjob_types.go"\u003eGo -\u0026gt;\u003c/a\u003e\u003c/p\u003e\n'},{id:27,href:"https://kubedl.io/docs/prologue/commands/",title:"Commands",description:"Commands for jobs",content:'\u003ch3 id="training-job-kind"\u003eTraining Job kind\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003etfjob\u003c/li\u003e\n\u003cli\u003epytorchjob\u003c/li\u003e\n\u003cli\u003emarsjob\u003c/li\u003e\n\u003cli\u003empijob\u003c/li\u003e\n\u003cli\u003exdljob\u003c/li\u003e\n\u003cli\u003eelasticdljob\u003c/li\u003e\n\u003cli\u003exgboostjob\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id="other-workload-kind"\u003eOther Workload Kind\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ecron\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThese kinds can be used in the kubectl command.\u003c/p\u003e\n\u003ch3 id="submit"\u003eSubmit\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003ekubectl apply -f https://raw.githubusercontent.com/alibaba/kubedl/v0.3.0/example/tf/tf_job_mnist.yaml\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3 id="list"\u003eList\u003c/h3\u003e\n\u003cdiv class="doks-clipboard"\u003e\n  \u003cbutton class="btn-clipboard btn btn-link" data-clipboard-text="kubectl get tfjobs -n kubedl"\u003e\u003cspan class="copy-status"\u003e\u003c/span\u003e\u003c/button\u003e\n\u003c/div\u003e\n\n\u003cpre\u003e\u003ccode class="language-bash"\u003ekubectl get tfjobs -n kubedl\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3 id="get"\u003eGet\u003c/h3\u003e\n\u003cdiv class="doks-clipboard"\u003e\n  \u003cbutton class="btn-clipboard btn btn-link" data-clipboard-text="kubectl describe tfjob mnist -n kubedl"\u003e\u003cspan class="copy-status"\u003e\u003c/span\u003e\u003c/button\u003e\n\u003c/div\u003e\n\n\u003cpre\u003e\u003ccode class="language-bash"\u003ekubectl describe tfjob mnist -n kubedl\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3 id="delete"\u003eDelete\u003c/h3\u003e\n\u003cdiv class="doks-clipboard"\u003e\n  \u003cbutton class="btn-clipboard btn btn-link" data-clipboard-text="kubectl delete tfjob mnist -n kubedl"\u003e\u003cspan class="copy-status"\u003e\u003c/span\u003e\u003c/button\u003e\n\u003c/div\u003e\n\n\u003cpre\u003e\u003ccode class="language-bash"\u003ekubectl delete tfjob mnist -n kubedl\n\u003c/code\u003e\u003c/pre\u003e\n'},{id:28,href:"https://kubedl.io/docs/recipes/events-persistency/",title:"Events Persistency",description:"",content:'\u003cp\u003eKubernetes object events are persisted for only 3 hours by default.\nIn addition to job meta persistency, KubeDL also supports persisting Kubernetes events into external storage system (usually time-series databases) to outlive api-server state.\nCurrently, KubeDL watches all Kubernetes events and persist them into external storage.\u003c/p\u003e\n\u003cp\u003eCurrently, only \u003ccode\u003ealiyun-sls\u003c/code\u003e is supported.\u003c/p\u003e\n\u003ch2 id="how-to-use"\u003eHow To Use\u003c/h2\u003e\n\u003cp\u003eBelow is an example for seting up KubeDL to persist events into \u003ca href="https://cn.aliyun.com/product/sls"\u003ealicloud simple log service\u003c/a\u003e.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eSet up credentials. Create a Secret:\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode class="language-yaml"\u003eapiVersion: v1\nkind: Secret\nmetadata:\n  name: kubedl-sls-config\n  namespace: kubedl-system\ntype: Opaque\nstringData:\n  endpoint: zhangbei.log.aliyuncs.com\n  accessKey: my-ak\n  accessSecret: my-sk\n  project: kubedl-project\n  logStore: kubedl\n\u003c/code\u003e\u003c/pre\u003e\n\u003col start="2"\u003e\n\u003cli\u003eUpdate the KubeDL Deployment spec to include \u003ccode\u003e--event-storage aliyun-sls\u003c/code\u003e in the startup flag and reference the credentials\nvia environment variables.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode class="language-yaml"\u003eapiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kubedl\n  namespace: kubedl-system\n  labels:\n    app: kubedl\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: kubedl\n  template:\n    metadata:\n      labels:\n        app: kubedl\n    spec:\n      containers:\n      - image: kubedl/kubedl:v0.3.0\n        imagePullPolicy: Always\n        name: kubedl-manager\n        args:\n        - \u0026quot;--event-storage\u0026quot;\n        - \u0026quot;aliyun-sls\u0026quot;\n        env:\n        - name: SLS_ENDPOINT\n          value:\n          valueFrom:\n            secretKeyRef:\n              name: kubedl-sls-config\n              key: endpoint\n        - name: SLS_KEY_ID\n          value:\n          valueFrom:\n            secretKeyRef:\n              name: kubedl-sls-config\n              key: accessKey\n        - name: SLS_KEY_SECRET\n          value:\n          valueFrom:\n            secretKeyRef:\n              name: kubedl-sls-config\n              key: accessSecret\n        - name: SLS_PROJECT\n          value:\n          valueFrom:\n            secretKeyRef:\n              name: kubedl-sls-config\n              key: project\n        - name: SLS_LOG_STORE\n          value:\n          valueFrom:\n            secretKeyRef:\n              name: kubedl-sls-config\n              key: logStore\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2 id="aliyun-sls-config"\u003eAliyun-sls Config\u003c/h2\u003e\n\u003cp\u003eThe configs defined in the aforementioned \u003ccode\u003esecret\u003c/code\u003e:\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eConfig Name\u003c/th\u003e\n\u003cth\u003eDescription\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003eendpoint\u003c/td\u003e\n\u003ctd\u003eThe sls endpoint to connect to\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eaccessKey\u003c/td\u003e\n\u003ctd\u003eThe alicloud account access key\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eaccessSecret\u003c/td\u003e\n\u003ctd\u003eThe alicloud account access secret.\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eproject\u003c/td\u003e\n\u003ctd\u003eThe sls project for storing the events\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003elogStore\u003c/td\u003e\n\u003ctd\u003eThe sls log store in the project for storing the events\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch2 id="contributions"\u003eContributions\u003c/h2\u003e\n\u003cp\u003eCurrently, only \u003ccode\u003ealiyun-sls\u003c/code\u003e is supported. You are welcome to contribute your own storage plugin.\u003c/p\u003e\n'},{id:29,href:"https://kubedl.io/docs/help/troubleshooting/",title:"Troubleshooting",description:"Solutions to common problems.",content:'\u003ch2 id="logs"\u003eLogs\u003c/h2\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003ekubectl logs kubedl-controller-manager-0 -n kubedl-system\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2 id="pod-status"\u003ePod status\u003c/h2\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003ekubectl describe pod kubedl-controller-manager-0 -n kubedl-system\n\u003c/code\u003e\u003c/pre\u003e\n'},{id:30,href:"https://kubedl.io/docs/recipes/dashboard/",title:"Dashboard",description:"A Dashboard for KubeDL",content:'\u003cfigure class="border-0"\u003e\n  \u003cimg class="img-fluid lazyload blur-up" data-sizes="auto" src="https://kubedl.io/docs/recipes/dashboard/blurred_hudf3e2b98e1f5f3cd8670feb126f8c27c_674843_20x0_resize_box_2.png" data-srcset="https://kubedl.io/docs/recipes/dashboard/blurred_hudf3e2b98e1f5f3cd8670feb126f8c27c_674843_900x0_resize_box_2.png 900w,https://kubedl.io/docs/recipes/dashboard/blurred_hudf3e2b98e1f5f3cd8670feb126f8c27c_674843_800x0_resize_box_2.png 800w,https://kubedl.io/docs/recipes/dashboard/blurred_hudf3e2b98e1f5f3cd8670feb126f8c27c_674843_700x0_resize_box_2.png 700w,https://kubedl.io/docs/recipes/dashboard/blurred_hudf3e2b98e1f5f3cd8670feb126f8c27c_674843_600x0_resize_box_2.png 600w,https://kubedl.io/docs/recipes/dashboard/blurred_hudf3e2b98e1f5f3cd8670feb126f8c27c_674843_500x0_resize_box_2.png 500w" width="3364" height="1722" alt="dashboard"\u003e\n  \u003cnoscript\u003e\u003cimg class="img-fluid" sizes="100vw" srcset="https://kubedl.io/docs/recipes/dashboard/blurred_hudf3e2b98e1f5f3cd8670feb126f8c27c_674843_900x0_resize_box_2.png 900w,https://kubedl.io/docs/recipes/dashboard/blurred_hudf3e2b98e1f5f3cd8670feb126f8c27c_674843_800x0_resize_box_2.png 800w,https://kubedl.io/docs/recipes/dashboard/blurred_hudf3e2b98e1f5f3cd8670feb126f8c27c_674843_700x0_resize_box_2.png 700w,https://kubedl.io/docs/recipes/dashboard/blurred_hudf3e2b98e1f5f3cd8670feb126f8c27c_674843_600x0_resize_box_2.png 600w,https://kubedl.io/docs/recipes/dashboard/blurred_hudf3e2b98e1f5f3cd8670feb126f8c27c_674843_500x0_resize_box_2.png 500w" src="https://kubedl.io/docs/recipes/dashboard/blurred.png" width="3364" height="1722" alt="dashboard"\u003e\u003c/noscript\u003e\n  \u003cfigcaption class="figure-caption"\u003eKubeDL Dashboard\u003c/figcaption\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eKubeDL dashboard consists of a frontend and a backend. Below documentation describes how to build and run them.\u003c/p\u003e\n\u003ch2 id="prerequisites"\u003ePrerequisites\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eNodeJS \u0026gt; 10\u003c/li\u003e\n\u003cli\u003eGo \u0026gt; 1.12\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id="deployment-guide"\u003eDeployment Guide\u003c/h2\u003e\n\u003ch3 id="deploy-the-kubedl-dashboard"\u003eDeploy the KubeDL Dashboard\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003ekubectl apply -f console/dashboard.yaml\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis will create a kubedl-dashboard \u003ccode\u003eDeployment\u003c/code\u003e, its \u003ccode\u003eService\u003c/code\u003e, and a \u003ccode\u003eConfigMap\u003c/code\u003e in the \u003ccode\u003ekubedl-system\u003c/code\u003e namespace.\u003c/p\u003e\n\u003cp\u003eThe dashboard will list nodes. Hence, its service account requires the \u003ccode\u003elist node permission\u003c/code\u003e.\u003c/p\u003e\n\u003ch3 id="access-the-dashboard"\u003eAccess the Dashboard\u003c/h3\u003e\n\u003cp\u003eYou can access the dashboard by the ClusterIP or LoadBalancer IP or Ingress depending on your own usage.\u003c/p\u003e\n\u003cp\u003eFor example, check the dashboard endpoint by inspecting the service object and you can find the access endpoint.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003e kubectl describe service kubedl-dashboard-service -n kubedl-system\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4 id="access-the-dashboard-over-ssh"\u003eAccess the Dashboard over SSH\u003c/h4\u003e\n\u003cp\u003eIf the dashboard is deployed on a remote machine that requires SSH to access. Run\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003essh -L 9090:localhost:9090 30.30.30.30\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis will send any browser connection to port 9090 on your local machine(i.e. your laptop), over ssh to the remote machine (30.30.30.30).\nOnce there, it will continue to localhost (the remote machine), port 9090.\u003c/p\u003e\n\u003cp\u003eThen, on the remote machine, run\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003e kubectl port-forward deployment/kubedl-dashboard -n kubedl-system 9090:9090\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis will forward any connections to localhost:9090 (the remote machine you ssh to) to the kubedl-dashboard deployment in Kuberenetes at port 9090\u003c/p\u003e\n\u003cp\u003eIn summary, the connection is flow like below:\u003c/p\u003e\n\u003cp\u003eBrowser -\u0026gt; Local Machine (e.g. your laptop), port 9090 -\u0026gt; Remote Machine, port 9090 -\u0026gt; kubectl forward -\u0026gt; The running pod, port 9090\u003c/p\u003e\n\u003ch2 id="development-guide"\u003eDevelopment Guide\u003c/h2\u003e\n\u003ch3 id="build-the-kubedl-dashboard-image"\u003eBuild the KubeDL Dashboard Image\u003c/h3\u003e\n\u003cpre\u003e\u003ccode\u003edocker build . -t kubedl/dashboard:0.1.0 -f Dockerfile.dashboard\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3 id="build-backend-server-binary"\u003eBuild Backend Server Binary\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003e$ cd console/\n$ go build -o backend-server ./backend/cmd/backend-server/main.go\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3 id="run-backend-server-locally"\u003eRun Backend Server Locally\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eCreate a \u003ccode\u003ekubedl-system\u003c/code\u003e namespace in your Kubernetes if not existing, this is required to create system-level ConfigMaps.\u003c/li\u003e\n\u003cli\u003eMake sure the backend-server uses a \u003ccode\u003eKUBECONFIG\u003c/code\u003e that has permission to create ConfigMap.\u003c/li\u003e\n\u003cli\u003eRun backend server with no authentication (default mode).\n\u003cpre\u003e\u003ccode class="language-bash"\u003eexport KUBECONFIG=\u0026lt;path/to/your/kubeconfig\u0026gt; \u0026amp;\u0026amp; ./backend-server\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch4 id="optional-settings"\u003eOptional Settings\u003c/h4\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eDefault Training Container Images\u003c/p\u003e\n\u003cp\u003eYou can set the default container images for submitting the training jobs through dashboard by creating a \u003ccode\u003eConfigMap\u003c/code\u003e\nnamed \u003ccode\u003ekubedl-dashboard-config\u003c/code\u003e in \u003ccode\u003ekubedl-system\u003c/code\u003e namespace as below:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-yaml"\u003e apiVersion: v1\n kind: ConfigMap\n metadata:\n     namespace: kubedl-system\n     name: kubedl-dashboard-config #\n data:\n     images: \'{\n         \u0026quot;tf-cpu-images\u0026quot;:[\n           \u0026quot;here set your default container image\u0026quot;,\n           ...\n         ],\n        \u0026quot;tf-gpu-images\u0026quot;:[\n           ...\n        ],\n        \u0026quot;pytorch-gpu-images\u0026quot;:[\n           ...\n        ]\n     }\'\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eAuthentication\u003c/p\u003e\n\u003cp\u003eBy default, the backend-server has no authentication.\nOptionally, you can enable authentication using ConfigMap. That is, use \u003ccode\u003eusername\u003c/code\u003e and \u003ccode\u003epassword\u003c/code\u003e defined in ConfigMap and to login.\u003c/p\u003e\n\u003cp\u003eThe backend-server needs to start as \u003ccode\u003e./backend-server --authentication-mode=config\u003c/code\u003e.\nFor example, create a ConfigMap like below:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-yaml"\u003eapiVersion: v1\n kind: ConfigMap\n metadata:\n     namespace: kubedl-system\n     name: kubedl-dashboard-config\n data:\n    images:\n           ...\n    users: \'[\n        {\n        \u0026quot;username\u0026quot;:\u0026quot;admin\u0026quot;,\n        \u0026quot;password\u0026quot;:\u0026quot;123456\u0026quot;\n        }\n    ]\'\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWhen login to the frontend, use \u003ccode\u003eadmin\u003c/code\u003e for username and \u003ccode\u003e123456\u003c/code\u003e for password to login.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id="run-frontend"\u003eRun Frontend\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eGo to the frontend root dir.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003ecd frontend/\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eInstall dependencies\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003enpm install\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eBuild Frontend Server\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003enpm run build\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eStart Frontend Server\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003enpm start\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch4 id="optional-set-backend-server-address"\u003eOptional: Set backend server address\u003c/h4\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eUse below config to set the backend-server address.\u003c/p\u003e\n\u003cp\u003ePath: console/frontend/config/config.js\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-javascript"\u003e  proxy: [\n    {\n      target: \u0026quot;http://localhost:9090\u0026quot;,\n      ...\n    }\n  ]\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eChange the target to your own backend server address. By default, it is \u003ccode\u003elocalhost:9090\u003c/code\u003e.\u003c/p\u003e\n\u003ch3 id="editor-tool-recommandation"\u003eEditor Tool Recommandation\u003c/h3\u003e\n\u003cp\u003eVSCode + ESlint(Plugin)\u003c/p\u003e\n\u003cp\u003eVSCode Configuration:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-javascript"\u003e{\n    \u0026quot;eslint.run\u0026quot;: \u0026quot;onSave\u0026quot;,\n    \u0026quot;eslint.format.enable\u0026quot;: true,\n    \u0026quot;editor.formatOnSave\u0026quot;: true,\n    \u0026quot;editor.codeActionsOnSave\u0026quot;: {\n        \u0026quot;source.fixAll.eslint\u0026quot;: true\n    }\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3 id="check-code-style"\u003eCheck code style\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003enpm run lint\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eYou can also use script to auto fix some lint error:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003enpm run lint:fix\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3 id="test-code"\u003eTest code\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003enpm test\n\u003c/code\u003e\u003c/pre\u003e\n'},{id:31,href:"https://kubedl.io/docs/help/",title:"Help",description:"Help KubeDL.",content:""},{id:32,href:"https://kubedl.io/docs/recipes/scheduling/",title:"Gang Scheduling",description:"",content:'\u003cp\u003eGang Scheduling is a critical feature for Deep Learning workloads to enable all-or-nothing scheduling capability, as\nmost DL frameworks requires all workers to be running to start training process. Gang Scheduling avoids resource\ninefficiency and scheduling deadlock sometimes.\u003c/p\u003e\n\u003cp\u003eKubeDL supports gang scheduling with different schedulers as a backend. Today, several Kubernetes schedulers support\ngang scheduling, including the \u003ca href="https://github.com/kubernetes-sigs/scheduler-plugins/blob/master/pkg/coscheduling/README.md"\u003eCoscheduling Scheduling Plugin\u003c/a\u003e,\n\u003ca href="https://yunikorn.apache.org/"\u003eYuniKorn\u003c/a\u003e, KubeBatch. Each has its own advantages and its own API protocols.\u003c/p\u003e\n\u003cp\u003eKubeDL provides a plugin framework to support different schedulers as a backend. Currently, KubeDL supports kube-coscheduler and kube-batch.\u003c/p\u003e\n\u003cfigure\u003e\n  \u003cimg class="img-fluid lazyload blur-up" data-sizes="auto" src="https://kubedl.io/docs/recipes/scheduling/kubedl-gangschedule_hue41869a04730efbd0fff875e875b8232_119795_20x0_resize_box_2.png" data-srcset="https://kubedl.io/docs/recipes/scheduling/kubedl-gangschedule_hue41869a04730efbd0fff875e875b8232_119795_900x0_resize_box_2.png 900w,https://kubedl.io/docs/recipes/scheduling/kubedl-gangschedule_hue41869a04730efbd0fff875e875b8232_119795_800x0_resize_box_2.png 800w,https://kubedl.io/docs/recipes/scheduling/kubedl-gangschedule_hue41869a04730efbd0fff875e875b8232_119795_700x0_resize_box_2.png 700w,https://kubedl.io/docs/recipes/scheduling/kubedl-gangschedule_hue41869a04730efbd0fff875e875b8232_119795_600x0_resize_box_2.png 600w,https://kubedl.io/docs/recipes/scheduling/kubedl-gangschedule_hue41869a04730efbd0fff875e875b8232_119795_500x0_resize_box_2.png 500w" width="874" height="572" alt="kubedl gang"\u003e\n  \u003cnoscript\u003e\u003cimg class="img-fluid" sizes="100vw" srcset="https://kubedl.io/docs/recipes/scheduling/kubedl-gangschedule_hue41869a04730efbd0fff875e875b8232_119795_900x0_resize_box_2.png 900w,https://kubedl.io/docs/recipes/scheduling/kubedl-gangschedule_hue41869a04730efbd0fff875e875b8232_119795_800x0_resize_box_2.png 800w,https://kubedl.io/docs/recipes/scheduling/kubedl-gangschedule_hue41869a04730efbd0fff875e875b8232_119795_700x0_resize_box_2.png 700w,https://kubedl.io/docs/recipes/scheduling/kubedl-gangschedule_hue41869a04730efbd0fff875e875b8232_119795_600x0_resize_box_2.png 600w,https://kubedl.io/docs/recipes/scheduling/kubedl-gangschedule_hue41869a04730efbd0fff875e875b8232_119795_500x0_resize_box_2.png 500w" src="https://kubedl.io/docs/recipes/scheduling/kubedl-gangschedule.png" width="874" height="572" alt="kubedl gang"\u003e\u003c/noscript\u003e\n  \u003cfigcaption class="figure-caption"\u003ekubedl gang schedule\u003c/figcaption\u003e\n\u003c/figure\u003e\n\n\u003ch3 id="how-to-enable"\u003eHow to Enable\u003c/h3\u003e\n\u003cp\u003eEnable gang scheduling using the KubeDL controller startup flag \u003ccode\u003e--gang-scheduler-name\u003c/code\u003e.\nFor example: \u003ccode\u003e--gang-scheduler-name=kube-coscheduler\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eBy default, it is empty meaning not enabled. Supported values are \u003ccode\u003ekube-coscheduler\u003c/code\u003e and \u003ccode\u003ekube-batch\u003c/code\u003e.\u003c/p\u003e\n\u003ch3 id="reference"\u003eReference\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eHow to setup kube-scheduler plugins. \u003ca href="https://github.com/kubernetes-sigs/scheduler-plugins"\u003eGo -\u0026gt;\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eHow to setup kube-batch scheduler. \u003ca href="https://github.com/kubernetes-sigs/kube-batch"\u003eGo -\u0026gt;\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n'},{id:33,href:"https://kubedl.io/docs/workloads/mpi/",title:"Mpi",description:"",content:'\u003ch2 id="spec"\u003eSpec\u003c/h2\u003e\n\u003cp\u003eCheck the CRD definition. \u003ca href="https://github.com/alibaba/kubedl/blob/master/apis/training/v1alpha1/mpijob_types.go"\u003eGo -\u0026gt;\u003c/a\u003e\u003c/p\u003e\n'},{id:34,href:"https://kubedl.io/docs/workloads/cron/",title:"Cron Training",description:"",content:'\u003cp\u003eKubeDL supports running the training jobs periodically using the \u003ccode\u003eCron\u003c/code\u003e workload.\u003c/p\u003e\n\u003ch2 id="example"\u003eExample\u003c/h2\u003e\n\u003cpre\u003e\u003ccode class="language-yaml"\u003eapiVersion: apps.kubedl.io/v1alpha1\nkind: Cron\nmetadata:\n  name: hello-cron-tf\nspec:\n  schedule: \u0026quot;0 * * * *\u0026quot; # trigger tf training per hour.\n  concurrencyPolicy: Allow\n  historyLimit: 10\n  template:\n    apiVersion: kubeflow.org/v1\n    kind: TFJob\n    workload:\n      metadata:\n        generateName: cron-tensorflow\n      spec:\n          cleanPodPolicy: Running\n          successPolicy: AllWorkers\n          tfReplicaSpecs:\n            PS:\n              replicas: 1\n              restartPolicy: Never\n              template:\n                spec:\n                  containers:\n                    - name: tensorflow\n                      image: kubedl/tf-mnist-with-summaries:1.0\n                      command:\n                        - \u0026quot;python\u0026quot;\n                        - \u0026quot;/var/tf_mnist/mnist_with_summaries.py\u0026quot;\n                        - \u0026quot;--log_dir=/train/logs\u0026quot;\n                        - \u0026quot;--learning_rate=0.01\u0026quot;\n                        - \u0026quot;--batch_size=150\u0026quot;\n                      resources:\n                        limits:\n                          cpu: 2048m\n                          memory: 2Gi\n                        requests:\n                          cpu: 1024m\n                          memory: 1Gi          \n            Worker:\n              replicas: 3\n              restartPolicy: Never\n              template:\n                spec:\n                  containers:\n                    - name: tensorflow\n                      image: kubedl/tf-mnist-with-summaries:1.0\n                      command:\n                        - \u0026quot;python\u0026quot;\n                        - \u0026quot;/var/tf_mnist/mnist_with_summaries.py\u0026quot;\n                        - \u0026quot;--log_dir=/train/logs\u0026quot;\n                        - \u0026quot;--learning_rate=0.01\u0026quot;\n                        - \u0026quot;--batch_size=150\u0026quot;\n                      resources:\n                        limits:\n                          cpu: 2048m\n                          memory: 2Gi\n                        requests:\n                          cpu: 1024m\n                          memory: 1Gi\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2 id="crd-spec"\u003eCRD Spec\u003c/h2\u003e\n\u003cp\u003eCheck the CRD definition. \u003ca href="https://github.com/kubedl-io/kubedl/blob/master/apis/apps/v1alpha1/cron_types.go"\u003eGo -\u0026gt;\u003c/a\u003e\u003c/p\u003e\n'},{id:35,href:"https://kubedl.io/docs/",title:"Docs",description:"Docs Doks.",content:""}];b.add(c),userinput.addEventListener('input',e,!0),suggestions.addEventListener('click',f,!0);function e(){var g=this.value,e=b.search(g,5),f=suggestions.childNodes,h=0,i=e.length,c;for(suggestions.classList.remove('d-none'),e.forEach(function(b){c=document.createElement('div'),c.innerHTML='<a href><span></span><span></span></a>',a=c.querySelector('a'),t=c.querySelector('span:first-child'),d=c.querySelector('span:nth-child(2)'),a.href=b.href,t.textContent=b.title,d.textContent=b.description,suggestions.appendChild(c)});f.length>i;)suggestions.removeChild(f[h])}function f(){while(suggestions.lastChild)suggestions.removeChild(suggestions.lastChild);return!1}})()